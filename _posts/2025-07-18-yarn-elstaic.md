---
layout: default
title:  YARNè¶…å–å†…å­˜å®ç°
description: "Yarnå†…å­˜è¶…å–é…ç½®å®è·µ,æå‡é›†ç¾¤å†…å­˜åˆ©ç”¨ç‡15%,è¶…å–å†…å­˜20TB+çš„æœ€ä½³å®è·µ" 
keywords: yarn,å†…å­˜è¶…å–,cgroup,èµ„æºè°ƒåº¦,å¤§æ•°æ®
author: liangrui
date:2026-01-06
---

# YARNè¶…å–å†…å­˜å®ç°
## å®ç°YARNå†…å­˜è¶…å–é…ç½®åçš„æˆæœ
  æå‡é›†ç¾¤å†…å­˜åˆ©ç”¨ç‡15%,è¶…å–å†…å­˜20TB+çš„å®è·µ
### æ•ˆæœå¯¹æ¯”
 å¯ç”¨cgroupæ¥ç›‘æ§å†…å­˜åï¼Œå¯¹æ¯”nodemangeræœåŠ¡å™¨ï¼Œnm JVMå †å†…å­˜æ•ˆç‡æå‡ï¼ŒGCæ—¶é•¿å’ŒGCæ¬¡æ•°æ˜æ˜¾é™ä½  
![alt text](/image/yarn-elastic/10.png)
è§‚å¯Ÿåˆ°nmå‡çº§ä¸ºcgroupsç®¡ç†å†…å­˜åï¼Œæ•´ä¸ªå †å†…å­˜åº”ç”¨é™ä½å¾ˆå¤šï¼Œå¯ä»¥æŠŠnmå†…å­˜é™ä½2Gï¼Œï¼ˆ8-2=6Gï¼‰,å¹¶æŠŠè¿™2Gç»™åˆ°yarnåˆ†é…ä½¿ç”¨ã€‚ ä¾‹ï¼šys13_7è°ƒæ•´åçš„æ•ˆæœ  
![alt text](/image/yarn-elastic/11.png)
 åŸ101G+15=116Gï¼Œè¶…å–15G  |  cgroupsé™åˆ¶å†…å­˜ä¸ºï¼š103G   
![alt text](/image/yarn-elastic/12.png)

 yarnå¯åˆ†é…æ€»å†…å­˜æå‡20TBå·¦å³ï¼Œå½“å‰å¯¹æ¯”90å¤©å†å²æ•°æ®    
![alt text](/image/yarn-elastic/13.png)
ä¸šåŠ¡åˆ†é…çš„å†…å­˜å¯¹æ¯”ï¼Œä¾‹ï¼šæ ¸å¿ƒé˜Ÿåˆ— å½“å‰åˆ†é…å†…å­˜ä¹Ÿç›¸å¯¹å¤šäº†15%ã€‚å…¶å®ƒé˜Ÿåˆ—ä¸€æ ·æå‡15%  
![alt text](/image/yarn-elastic/14.png)
é«˜å³°æœŸä»»åŠ¡å¹¶è¡Œé‡å¢åŠ ï¼Œè¯´æ˜æœ‰æ›´å¤šçš„èµ„æºç»™åˆ°ä»»åŠ¡åœ¨è·‘  
![alt text](/image/yarn-elastic/15.png)


## Yarné»˜è®¤å†…å­˜ç®¡ç†å­˜åœ¨çš„é—®é¢˜

å½“å‰è§‚å¯ŸnodeMangerç‰©ç†æœºå®é™…å†…å­˜åˆ©ç”¨ç‡ï¼Œè¿˜å­˜åœ¨ä¸€å®šçš„æµªè´¹ï¼ˆspark åœ¨åˆ†é…ç½®å†…å­˜çš„æ—¶å€™ï¼Œå®é™…jvmå­˜åœ¨æ²¡æœ‰ç”¨æ»¡çš„æƒ…å†µï¼‰ï¼Œå­˜åœ¨ä¸€å®šçš„å†…å­˜ç¢ç‰‡ã€‚
ç›‘æ§è§‚æŸ¥ï¼ˆå¯ä»¥åœ¨å‡ ç§’å†…å†…å­˜æ˜¯å¯ä»¥ç”¨åˆ°100%åˆ©ç”¨ç‡çš„ï¼‰å¤§çº¦å¹³å‡åœ¨60%-80%ä½¿ç”¨ç‡ã€‚
![alt text](/image/yarn-elastic/yarn-server-memory.png)

### æµ‹è¯•éªŒè¯

|ç‰©ç†æœº|ç‰©ç†æœºå†…å­˜(G)|nmåˆ†é…å†…å­˜(G)|è·‘ä»»åŠ¡ä½¿ç”¨åˆ°100%ï¼ˆGï¼‰|ç‰©ç†æœºå†…å­˜åˆ©ç”¨ç‡|ç³»ç»Ÿé¢„ç•™|ç¢ç‰‡æµªè´¹|
|-|-|-|-|-|-|-|
|10.12.66.1|125|100|100%|28%|10%|50%|
|10.12.66.2|125|100|100%|28%|10%|50%|
|10.12.66.3|125|100|100%|28%|10%|50%|
|||åˆè®¡300G|||||
  
ç”¨sparkä»»åŠ¡æ‰“æ»¡æ­£å¸¸é€»è¾‘çš„åˆ†é…å†…å­˜

è¿™é‡Œä¸ºäº†éªŒè¯å¯è§‚æ€§ï¼Œæ•…æ„å†™äº†ä¸€ä¸ªå®é™…ç”¨å†…å­˜å¾ˆä½çš„ä»»åŠ¡ï¼Œä½†å†…å­˜ç»™çš„å¾ˆé«˜çš„æƒ…å†µä¸‹ã€‚

```
#è¿™é‡Œæˆ‘ä»¬æŒ‡å®šäº†executor-memory 12G,å®é™…æ¯ä¸ªexecutorçš„javaè¿›ç¨‹å†…å­˜å¼€é”€å¾ˆä½
  
/home/liangrui/spark/bin/spark-submit \
--class com.aengine.spark.app.example.HdfsTest  \
--conf spark.kerberos.access.hadoopFileSystems=hdfs://yycluster01,hdfs://yycluster02 \
--master yarn --deploy-mode cluster --executor-memory 12G  --driver-memory 1G \
--queue lion --principal test-hiido2 --keytab /home/liangrui/test-hiido2.keytab --name HdfsTest   \
--num-executors 5 /home/liangrui/spark/examples/jars/aengine-spark-3_2.jar  hdfs://yycluster01/user/dev/0225-11m-d.json
```
![alt text](/image/yarn-elastic/02.png)
![alt text](/image/yarn-elastic/03.png)
yarné›†ç¾¤æ˜¾ç¤ºå†…å­˜use 100GB,å®é™…ç‰©ç†å†…åªæœ‰28%çš„åˆ©ç”¨ç‡ï¼Œé‚£ä¹ˆå°±æœ‰50%çš„å†…å­˜å­˜åœ¨ç¢ç‰‡æµªè´¹
<!--  -->

### è§£å†³æ–¹æ¡ˆ  
è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œéœ€è¦å¼€å¯yarn cgroupåŠŸèƒ½  
ä»¥ä¸‹å†…å®¹æ˜¯DeepSeekç»™å‡ºçš„å¯¹æ¯”  é—®ç­”ä¸»é¢˜ï¼šï¼ˆlinux pid å†…å­˜ç»Ÿè®¡ vs cgroupå†…å­˜ç»Ÿè®¡ï¼‰ 
![alt text](/image/yarn-elastic/04.png)

## å®ç°æ–¹æ¡ˆ
### é˜¶æ®µä¸€
æˆ‘ä»¬å½“å‰hadoopç‰ˆæœ¬æ˜¯3.1.1,ç‰ˆæœ¬è¾ƒä½ï¼ŒYARN 3.2.0ä¸­ä½¿ç”¨å†…å­˜å¼¹æ€§æ§åˆ¶ï¼Œéœ€è¦æŠŠç›¸å…³PRåŠŸèƒ½åˆå¹¶å›3.1.1,å…·ä½“åˆå¹¶å†…å®¹
![alt text](/image/yarn-elastic/05.png)
#### å‚è€ƒæ–‡æ¡£
cgroupæ–‡æ¡£ï¼š	https://docs.kernel.org/admin-guide/cgroup-v1/cgroups.html  
CGroups ä¸ YARN ç»“åˆä½¿ç”¨ï¼š	https://hadoop.apache.org/docs/r3.3.0/hadoop-yarn/hadoop-yarn-site/NodeManagerCgroups.html  
YARN ä¸­ä½¿ç”¨å†…å­˜æ§åˆ¶ï¼š	https://hadoop.apache.org/docs/r3.3.0/hadoop-yarn/hadoop-yarn-site/NodeManagerCGroupsMemory.html  
cgroup å†…å­˜æº¢å‡ºæ§åˆ¶ä¾‹å­ï¼š	https://docs.redhat.com/zh-cn/documentation/red_hat_enterprise_linux/7/html/resource_management_guide/sec-memory#memory_example-usage  


### é˜¶æ®µäºŒ
å®ç°å®Œé˜¶æ®µä¸€åŠŸèƒ½åï¼Œåœ¨ä¸å¼€å¯ä»»ä½•é…ç½®ï¼ŒæŠŠjaråŒ…æ›´æ–°åˆ°é›†ç¾¤ä¸­ï¼ŒéªŒè¯åˆå¹¶ä»£ç æ˜¯å¦æ­£å¸¸ï¼Œæ­£å¸¸åè¿›è¡Œé˜¶æ®µäºŒé…ç½®ä¼˜åŒ–ã€‚

#### ä½†è¿˜éœ€è¦ä»¥ä¸‹æ”¹è¿›
- é˜¶æ®µä¸€æ˜¯ç›´æ¥è°ƒå°æ¯ä¸ªä½œä¸šçš„åˆ†é…å†…å­˜å¤§å°ï¼Œæ¥æ»¡è¶³è¶…å–å†…å­˜çš„éœ€æ±‚ï¼Œè¿™æ ·æ”¹åŠ¨èŒƒå›´è¾ƒå¤§ï¼Œä¸å¥½ç®¡ç†ï¼Œå› æ­¤åœ¨æœåŠ¡ç«¯è°ƒæ•´nmåˆ†é…å†…å­˜å’Œå®é™…ç›‘æ§å†…å­˜éš”ç¦»ã€‚
- è§¦å‘å®é™…ç‰©ç†å†…å­˜oom-killå¯¼è‡´ä½œä¸šå¼‚å¸¸åï¼Œå¯¹ä½œä¸šäº§ç”Ÿå¼‚å¸¸éœ€è¦æœ‰ä¸ªå…œåº•æ–¹æ¡ˆ
- è°ƒæ•´jvm Xmxé™åˆ¶ï¼Œç›®å‰sparkçš„jvm xmxå’Œexecutoræ˜¯ä¸€è‡´çš„ï¼Œhiveç”¨çš„æ˜¯mrå‚æ•°ï¼Œé»˜è®¤æ˜¯mapre 4gå’Œ-Xmx 4000mã€‚å¯ç”¨å¼¹æ€§å†…å­˜åï¼Œå¯ä»¥æ”¾å¤§æ­¤é™åˆ¶ã€‚

##### nmåˆ†é…ç½®å†…å­˜å’Œoomç›‘æ§å†…å­˜éš”ç¦»
è°ƒè¯•hadoopæºç å‘ç°è¿™é‡Œçš„é€»è¾‘å¯ä»¥å®ç°åˆ†ç¦»é…ç½®
![alt text](/image/yarn-elastic/06.png)

#### å®é™…å†…å­˜ä¸è¶³å¯¼è‡´ä½œä¸šå¼‚å¸¸
å­˜åœ¨é›†ç¾¤æŠŠå®é™…å†…å­˜ç”¨å®Œï¼Œä½†yarnè®¤ä¸ºè¿˜æœ‰å†…å­˜å¯ç”¨ï¼Œå°±ä¼šç»§ç»­åˆ†é…ç½®ä½œä¸šç»™nm,è¿™æ—¶å€™nmå°±ä¼šæŒ‰killæ‰container(æŒ‰ä¸€å®šçš„é¡ºåº)ï¼Œç›´åˆ°nmæœ‰å†…å­˜å¯ä»¥ç»§ç»­è·‘å…¶å®ƒcontainerã€‚
ä¾‹ï¼šä¸€å°yarnå¯åˆ†é…110Gå†…å­˜ï¼Œcgroupå®é™…å†…å­˜ä¸º88G=110*0.80f
hadoop   jar /home/liangrui/spark/examples/jars/hdfs-client-op-1.0-SNAPSHOT.jar test.SleepMr hdfs://yycluster01/user/dev/0225-11m-d.json   hdfs://yycluster01/user/dev/out/mr-0225-11m-d.txt 600 5 2048 10240m
å‚æ•°ï¼š5ä¸ªreduce*5GB+å†…å­˜å¼€é”€ï¼Œä½†yarnåˆ†é…ä¸º5ä¸ªreduce*2Gå†…å­˜ï¼Œå½“cgroupå†…å­˜å¼€é”€åˆ°88Gæ—¶ï¼Œnmå°±ä¼šè§¦å‘oom-kill,ä½œä¸šå°±ä¼šå¾—åˆ°ä»¥ä¸‹æ•ˆæœï¼Œcontainerä¸€ç›´åœ¨è¢«killï¼Œä¸€ç›´åœ¨é‡è¯•ï¼Œåªæœ‰AMä¸è¢«killæ‰å°±å¯ä»¥è¿›è¡Œé‡è¯•ï¼Œç›´åˆ°æœ‰å†…å­˜å¯ä»¥è·‘ã€‚
```
#MR AMé»˜è®¤é‡è¯•2æ¬¡
mapreduce.am.max-attempts=2
#YARN æœåŠ¡ç«¯AMé‡è¯•2æ¬¡
yarn.resourcemanager.am.max-attempts=2
#spark AMé‡è¯•1æ¬¡,å¯¹åº”yarn am
spark.yarn.maxAppAttempts=1

#yarn serviceç«¯é…ç½®ï¼Œå®ƒä»¬è¢«æœåŠ¡ AM æ›¿æ¢
yarn.service.am-râ€‹â€‹estart.max-attempts=20
yarn.service.container-failure.retry.max=-1  æ²¡æœ‰é™åˆ¶ã€‚
```

è¢«killæ‰çš„ä»»åŠ¡ä¸ç®—å¼‚å¸¸ï¼Œä½†MRè¿˜æœ‰ä¸€ç§ä¼šå¯¼è‡´å¼‚å¸¸ï¼Œåœ¨æ€»ä½“å®é™…å†…å­˜è¶…é™çš„æ—¶å€™ï¼Œä½œä¸šå†…éƒ¨å…¶å®ƒè¿›ç¨‹ï¼Œå¯èƒ½ç”³è¯·å†…å­˜å¯¼è‡´å†…å­˜ä¸è¶³å¼‚å¸¸(åœ¨oom-killå¤„ç†çš„åŒæ—¶è§¦å‘)ã€‚

ä¾‹:è¿™é‡Œåªæœ‰ä¸€å°nmçš„æƒ…å†µï¼Œå¹¶æŠŠnmå¯ç”¨çš„å®é™…å†…å­˜ç”¨æ»¡çš„æƒ…å†µï¼šç­‰äºçº¿ä¸Šå®é™…yarnå¯ç”¨å®é™…ç‰©ç†å†…å­˜ç”¨å®Œçš„æƒ…å†µã€‚ä¹Ÿå°±æ˜¯100%åˆ©ç”¨ç‡çš„æ—¶å€™ã€‚

```
#å®¢æˆ·ç«¯æ—¥å¿—
25/03/19 14:44:35 INFO mapreduce.Job: Task Id : attempt_1742358710616_0001_r_000003_2, Status : FAILED
[2025-03-19 14:44:32.093]Container killed on request. Exit code is 137
[2025-03-19 14:44:33.682]Container exited with a non-zero exit code 137. 
[2025-03-19 14:44:33.683]Killed by external signal

25/03/19 14:44:37 INFO mapreduce.Job:  map 100% reduce 13%
25/03/19 14:45:01 INFO mapreduce.Job: Task Id : attempt_1742358710616_0001_r_000004_2, Status : FAILED
[2025-03-19 14:44:58.572]Container killed on request. Exit code is 137
[2025-03-19 14:44:59.356]Container exited with a non-zero exit code 137. 
[2025-03-19 14:44:59.357]Killed by external signal

25/03/19 14:45:05 INFO mapreduce.Job: Task Id : attempt_1742358710616_0001_r_000001_2, Status : FAILED
[2025-03-19 14:45:02.436]Container killed on request. Exit code is 137
[2025-03-19 14:45:02.657]Container exited with a non-zero exit code 137. 
[2025-03-19 14:45:02.658]Killed by external signal

25/03/19 14:45:12 INFO mapreduce.Job:  map 100% reduce 100%
25/03/19 14:45:13 INFO mapreduce.Job: Job job_1742358710616_0001 failed with state FAILED due to: Task failed task_1742358710616_0001_r_000003
Job failed as tasks failed. failedMaps:0 failedReduces:1 killedMaps:0 killedReduces: 0

25/03/19 14:45:13 INFO mapreduce.Job: Counters: 40

#app master æ—¥å¿—
2025-03-19 14:45:10,145 DEBUG [IPC Server handler 24 on 29795] org.apache.hadoop.ipc.Server: IPC Server handler 24 on 29795: responding to Call#5 
Retry#1 fatalError(attempt_1742358710616_0001_r_000003_3, org.apache.hadoop.mapreduce.task.reduce.Shuffle$ShuffleError: Error while doing final merge 
    at org.apache.hadoop.mapreduce.task.reduce.Shuffle.run(Shuffle.java:160)
    at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:377)
    at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)
    at java.security.AccessController.doPrivileged(Native Method)
    at javax.security.auth.Subject.doAs(Subject.java:422)
    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
    at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)
Caused by: org.apache.hadoop.fs.FSError: java.io.IOException: Cannot allocate memory
    at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.write(RawLocalFileSystem.java:262)
    at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
    at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)
    at java.io.FilterOutputStream.flush(FilterOutputStream.java:140)
    at java.io.DataOutputStream.flush(DataOutputStream.java:123)
    at java.io.FilterOutputStream.flush(FilterOutputStream.java:140)
    at java.io.FilterOutputStream.flush(FilterOutputStream.java:140)
    at java.io.DataOutputStream.flush(DataOutputStream.java:123)
    at org.apache.hadoop.mapred.IFile$Writer.close(IFile.java:158)
    at org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl.finalMerge(MergeManagerImpl.java:759)
    at org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl.close(MergeManagerImpl.java:379)
    at org.apache.hadoop.mapreduce.task.reduce.Shuffle.run(Shuffle.java:158)
    ... 6 more
Caused by: java.io.IOException: Cannot allocate memory
    at java.io.FileOutputStream.writeBytes(Native Method)
    at java.io.FileOutputStream.write(FileOutputStream.java:326)
    at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.write(RawLocalFileSystem.java:260)

```
###### sparké€»è¾‘
yarnåœ¨åå°killæ‰containeråï¼Œsparkä¼šæ ‡è®°ä¸€æ¬¡å¼‚å¸¸(ä»¥stageé‡Œçš„indexä¸ºç»´åº¦æ ‡è®°ï¼Œä¸€ä¸ªindexé‡Œé‡è¯•4æ¬¡(å¯é…ç½®ï¼Œé»˜è®¤ä¸º4æ¬¡)éƒ½ä¸ºå¼‚å¸¸åï¼Œå°±ä¼šæŠŠä½œä¸šæ ‡è®°ä¸ºå¼‚å¸¸)ã€‚

ä¾‹ï¼šåªæœ‰ä¸€å°nodemangerä¸Šè·‘spark,æŠŠcgroupå¯åˆ†é…çš„å†…å­˜é‡å…¨éƒ¨ç”¨å®Œåï¼Œè¯´æ˜å®é™…å†…å­˜å·²ç”¨å®Œï¼Œä¼šè§¦å‘yarn oom-killæœºåˆ¶ã€‚

![](https://rte.weiyun.baidu.com/wiki/attach/image/api/imageDownloadAddress?attachId=e66f1f1e65ee448daba9dc4aaeca1d01&docGuid=m534AiztULZZIu "")
ä¸€æ ·çš„ä½œä¸šå†…å­˜å¼€é”€ä¸ºä¾‹ï¼Œæœ€åä¸€ä¸ªæäº¤çš„saprkä½œä¸šä¼šå¼‚å¸¸ï¼Œå¼‚å¸¸ä¿¡æ¯ä¸ºï¼š

```
# nodemangeræ—¥å¿—
2025-03-23 11:17:02,348 WARN  resources.DefaultOOMHandler (DefaultOOMHandler.java:killContainer(252)) - container container_e78_1742465778745_0034_02_000006 killed by elastic cgroups OOM handler.
2025-03-23 11:17:16,514 WARN  resources.DefaultOOMHandler (DefaultOOMHandler.java:killContainer(252)) - container container_e78_1742465778745_0034_02_000005 killed by elastic cgroups OOM handler.
2025-03-23 11:17:31,400 WARN  resources.DefaultOOMHandler (DefaultOOMHandler.java:killContainer(252)) - container container_e78_1742465778745_0034_02_000008 killed by elastic cgroups OOM handler.
2025-03-23 11:17:46,987 WARN  resources.DefaultOOMHandler (DefaultOOMHandler.java:killContainer(252)) - container container_e78_1742465778745_0034_02_000009 killed by elastic cgroups OOM handler.
2025-03-23 11:18:01,190 WARN  resources.DefaultOOMHandler (DefaultOOMHandler.java:killContainer(252)) - container container_e78_1742465778745_0034_02_000010 killed by elastic cgroups OOM handler.
2025-03-23 11:18:18,931 WARN  resources.DefaultOOMHandler (DefaultOOMHandler.java:killContainer(252)) - container container_e78_1742465778745_0034_02_000011 killed by elastic cgroups OOM handler.
2025-03-23 11:18:35,678 WARN  resources.DefaultOOMHandler (DefaultOOMHandler.java:killContainer(252)) - container container_e78_1742465778745_0034_02_000012 killed by elastic cgroups OOM handler.

# spark driveræ—¥å¿—
Driver stacktrace:
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 9) (on-test-hadoop-65-239.hiido.host.int.yy.com executor 10): ExecutorLostFailure (executor 10 exited caused by one of the running tasks) Reason: Container from a bad node: container_e78_1742465778745_0034_01_000012 on host: on-test-hadoop-65-239.hiido.host.int.yy.com. Exit status: 137. Diagnostics: [2025-03-23 11:16:35.969]Container killed on request. Exit code is 137
[2025-03-23 11:16:36.472]Container exited with a non-zero exit code 137. 
[2025-03-23 11:16:36.473]Killed by external signal
.
Driver stacktrace:
    at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)
    at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)
    at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)
    at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
    at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
    at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
    at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)
    at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)
    at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)
    at scala.Option.foreach(Option.scala:407)
    at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)
    at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
    at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2279)
    at org.apache.spark.rdd.RDD.count(RDD.scala:1253)
    at com.aengine.spark.app.example.HdfsTest10G$.main(HdfsTest10G.scala:64)
    at com.aengine.spark.app.example.HdfsTest10G.main(HdfsTest10G.scala)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:737)

```
 Task 0 in stage 0.0 failed 4 times, most recent failure: ï¼Œè¿™é‡Œçš„failed 4æ˜¯æŒ‡index 1é‡è¯•äº†4æ¬¡ï¼Œå…¶å®ƒindexä¹Ÿæœ‰å¤±è´¥ä½†è¿˜æ²¡åˆ°4æ¬¡ï¼Œåªæœ‰å…¶ä¸­ä¸€ä¸ªåˆ°4æ¬¡ä¸Šé™å°±ä¼šå…¨éƒ¨å¼‚å¸¸




##### Flinkå¼•æ“é€»è¾‘
https://nightlies.apache.org/flink/flink-docs-release-1.6/ops/deployment/yarn_setup.html
```
yarn.maximum-failed-containers = é»˜è®¤=-n  -nå°±æ˜¯TaskManager å®¹å™¨
ä¾‹ï¼š./bin/yarn-session.sh -n 5  # è¯·æ±‚ 5 ä¸ª TaskManager å®¹å™¨ æ­¤æ—¶ï¼Œyarn.maximum-failed-containers çš„é»˜è®¤å€¼å³ä¸º 5ã€‚è‹¥å¤±è´¥å®¹å™¨æ•°è¶…è¿‡æ­¤å€¼ï¼ŒYARN ä¼šè¯å°†ç»ˆæ­¢ã€‚
```

##### mr&spark taskå¼‚å¸¸é…ç½®

```
#MR
mapreduce.map.failures.maxpercent=0
mapreduce.reduce.failures.maxpercent=0
#spark
spark.task.maxFailures=4
```


#### å¯¼è‡´å¼‚å¸¸è§£å†³æ–¹æ¡ˆ
1ï¼šå¦‚æœè¿™ç§å¼‚å¸¸å¤§é¢ç§¯å‡ºç°ï¼Œå°±éœ€è¦è°ƒå°cgroupé™åˆ¶å†…å­˜å’Œyarné¢„åˆ†é…çš„å†…å­˜å·®è·(yarn.nodemanager.elastic-memory-control.ratio)ï¼Œè®©é¢„åˆ†é…å†…å­˜å’Œå®é™…å†…å­˜æ›´ç›¸è¿‘ï¼Œè®©å†…å­˜ä¸èƒ½è¶…å–å¤ªå¤šã€‚
2ï¼šå¯ç”¨yarnæœºä¼šå‹ä½œä¸šï¼Œæ ¹æ®ä¸šåŠ¡ç­‰çº§æŠŠä¸é‡è¦çš„é˜Ÿåˆ—è®¾ç½®ä¸ºæœºä¼šå‹ä½œä¸šï¼ˆæœºä¼šå‹ï¼šä¼šé¢„å…ˆåˆ†é…å¹¶åˆå§‹åŒ–ä½œä¸šåŠ è½½ï¼Œåªæ˜¯ç­‰å†…å­˜è¿è¡Œï¼Œä¸€ä½†å…¶å®ƒcontaineré‡Šæ”¾å†…å­˜åï¼Œç«‹åˆ»è®¡ç®—ï¼Œä½†åœ¨å†…å­˜ä¸è¶³çš„æ—¶å€™ï¼Œä¼šä¼˜å…ˆkillæ‰è¿™ä¸ªç±»å‹çš„ä½œä¸šï¼‰åœ¨ä¸€å®šç¨‹åº¦ä¸Šå‡å°‘å¯¹æ­£å¸¸ä½œä¸šå‡ºé”™çš„æ¦‚ç‡ï¼Œå¹¶ä¼šåŠ å¿«ä½œä¸šè¿è¡Œå®Œï¼Œè¿˜æœ‰ä¸ªå¥½å¤„æ˜¯ï¼š
ä¾‹ï¼šæŠŠdapè®¾ä¸ºæœºä¼šç±»å‹ä½œä¸šï¼Œåœ¨èµ„æºæ»¡çš„æ—¶å€™ï¼Œä¼šè¢«æ­£å¸¸ä½œä¸škillæ‰ï¼Œè¿™æ ·ä¸ç”¨æå¿ƒdapæäº¤å¤§ä»»åŠ¡ï¼Œä¸€ç›´è·‘åˆ°æ™šä¸Šï¼Œå ç”¨æ™šä¸Šé«˜å³°æœŸèµ„æºæ€§å†µå‡ºç°ã€‚

3:è§£å†³ï¼šåœ¨sparkè¯·æ±‚yarnå®¹å™¨çš„æ—¶å€™ï¼Œå¢åŠ ä¸€ä¸ªè®¡æ•°å™¨ï¼Œä½†è¿™ä¸ªè®¡æ•°å™¨åªèƒ½ä»¥appidç»´åº¦è®¡æ•°ï¼Œä¸èƒ½åšåˆ°taskçº§åˆ«killè®¡æ•°ï¼Œtadkçº§åˆ«æ•°æ®åœ¨sparkå†…æ ¸ä¸­å®ç°
ä¹Ÿå°±æ˜¯è¿™ä¸ªä½œä¸šçš„taskè¢«killåˆ°Næ¬¡åï¼Œå°±å¼ºåˆ¶åé¢æ‰€æœ‰çš„containerè¯·æ±‚è‡ªåŠ¨å‡çº§ä¸ºGUARANTEEDï¼Œè¿™æ ·yarnå°±ä¸ä¼šä¼˜å…ˆKillè¿™ä¸ªä½œä¸šï¼Œè€Œæ˜¯æ‰¾è¶…å‡ºå†…å­˜çš„æˆ–æ—¶é—´åé¢èµ·çš„containerè¿›è¡Œkill.é¿å…ä½œä¸šå¼‚å¸¸ã€‚
ä»¥ä¸‹æ˜¯æ”¹åŠ¨äº†saprk æºç å®ç°ï¼Œåœ¨è¢«killæ‰Næ¬¡åï¼Œä¸»åŠ¨å‡çº§ä¸ºGUARANTEEDç±»å‹ã€‚
```
spark.task.oomKill.maxFailures=é»˜è®¤æºç é‡ŒæŒ‡å®šåˆ°30æ¬¡,å¯æ ¹æ®æƒ…å†µè°ƒå¤§,ä¹‹æ‰€æœ‰å¯ä»¥è°ƒåˆ°è¶…4æ¬¡ï¼Œæ˜¯å› ä¸ºæŠŠcode 137å‰”é™¤äº† spark.task.maxFailuresçš„è®¡æ•°
```
å…·ä½“ä»£ç æ”¹åŠ¨å‚è€ƒï¼šhttps://issues.apache.org/jira/secure/attachment/13076601/spark-support-yarn-OPPORTUNISTIC.patch
![alt text](/image/yarn-elastic/08.png)
å¯ä»¥çœ‹åˆ°åé¢çš„ç±»å‹å·²å…¨ä¸ºä¿è¯ç±»å‹ï¼Œä¸ä¼šä¼˜å…ˆè¢«killäº†ã€‚
![alt text](/image/yarn-elastic/09.png)

4ï¼šå¯ç”¨yarnæœºä¼šå‹ä½œä¸šï¼Œæ ¹æ®ä¸šåŠ¡ç­‰çº§æŠŠä¸é‡è¦çš„é˜Ÿåˆ—è®¾ç½®ä¸ºæœºä¼šå‹ä½œä¸šï¼ˆæœºä¼šå‹ï¼šä¼šé¢„å…ˆåˆ†é…å¹¶åˆå§‹åŒ–ä½œä¸šåŠ è½½ï¼Œåªæ˜¯ç­‰å†…å­˜è¿è¡Œï¼Œä¸€ä½†å…¶å®ƒcontaineré‡Šæ”¾å†…å­˜åï¼Œç«‹åˆ»è®¡ç®—ï¼Œä½†åœ¨å†…å­˜ä¸è¶³çš„æ—¶å€™ï¼Œä¼šä¼˜å…ˆkillæ‰è¿™ä¸ªç±»å‹çš„ä½œä¸šï¼‰åœ¨ä¸€å®šç¨‹åº¦ä¸Šå‡å°‘å¯¹æ­£å¸¸ä½œä¸šå‡ºé”™çš„æ¦‚ç‡ï¼Œå¹¶ä¼šåŠ å¿«ä½œä¸šè¿è¡Œå®Œï¼Œè¿˜æœ‰ä¸ªå¥½å¤„æ˜¯ï¼š

ä¾‹ï¼šæŠŠdapè®¾ä¸ºæœºä¼šç±»å‹ä½œä¸šï¼Œåœ¨èµ„æºæ»¡çš„æ—¶å€™ï¼Œä¼šè¢«æ­£å¸¸ä½œä¸škillæ‰ï¼Œè¿™æ ·ä¸ç”¨æå¿ƒdapæäº¤å¤§ä»»åŠ¡ï¼Œä¸€ç›´è·‘åˆ°æ™šä¸Šï¼Œå ç”¨æ™šä¸Šé«˜å³°æœŸèµ„æºæ€§å†µå‡ºç°ã€‚
 Task 0 in stage 0.0 failed 4 times, most recent failure: ï¼Œè¿™é‡Œçš„failed 4æ˜¯æŒ‡index 1é‡è¯•äº†4æ¬¡ï¼Œå…¶å®ƒindexä¹Ÿæœ‰å¤±è´¥ä½†è¿˜æ²¡åˆ°4æ¬¡ï¼Œåªæœ‰å…¶ä¸­ä¸€ä¸ªåˆ°4æ¬¡ä¸Šé™å°±ä¼šå…¨éƒ¨å¼‚å¸¸

![alt text](/image/yarn-elastic/07.png)
5ï¼šYARNå±‚é¢ä¼˜åŒ– 
 åœ¨è§¦å‘cgroup oom-killçš„æ—¶å€™ï¼Œä¿®æ”¹æºç ï¼Œåˆ¤æ–­containeræ˜¯å¦æ˜¯master,émasterçš„containeræ‰ä¼šè¢«oom-kill,ä¸ç„¶ä¼šå¯¼è‡´ä½œä¸šå¤±è´¥

5:å‘Šè­¦æœºåˆ¶
  - å¢åŠ cgroups ook-killæ¬¡æ•°ç›‘æ§æŒ‡æ ‡ï¼Œåœ¨hadoopæºç é‡Œå¢åŠ ï¼Œç›®å‰hadoopæºç æ²¡æœ‰å®ç°è¿™ä¸ªæŒ‡æ ‡ç›‘æ§ã€‚å¯ä»¥åŠæ—¶å‘ç°è¶…å–å¼‚å¸¸é—®é¢˜ã€‚
  - ç›¸å…³å¼‚å¸¸exit codeä½œä¸šå¼‚å¸¸å‘Šè­¦,ä¸»è¦æ˜¯13,137ï¼Œå†…å­˜é—®é¢˜å¯¼è‡´åŠ è½½ç±»å¤±è´¥ç­‰å¼‚å¸¸


<div class="post-date">
  <span class="calendar-icon">ğŸ“…</span>
  <span class="date-label">å‘å¸ƒï¼š</span>
  <time datetime="2025-07-18" class="date-value">2025-07-18</time>
</div>


<div class="outline" style="background:#f6f8fa;padding:1em 1.5em 1em 1.5em;margin-bottom:2em;border-radius:8px;">
  <strong>å¤§çº²ï¼š</strong>
  <ul id="outline-list" style="margin:0;padding-left:1.2em;"></ul>
</div>


<!--èœå•æ -->
  <nav class="blog-nav">
    <button class="collapse-btn" onclick="toggleBlogNav()">â˜°</button>
    {% include blog_navigation.html items=site.data.blog_navigation %}
 </nav>

 <script src="/assets/blog.js"></script>
<link rel="stylesheet" href="/assets/blog.css">