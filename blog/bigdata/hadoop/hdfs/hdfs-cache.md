---
layout: default
title:  hdfs cache
author: liangrui
description: "hdfs cache" 
keywords: hadoop,hdfs,HDFS MemoryStorage,CentralizedCacheManagement
date: 2025-11-20
---

# hdfsé›†ä¸­å¼ç¼“å­˜
hdfsä¹Ÿæ”¯æŒç¼“å­˜é…ç½®ï¼ŒæŠŠå†…å­˜å½“ä½œç£ç›˜ä¸€æ ·è¯»å†™æ•°æ®ï¼Œç±»ä¼¼alluxio cache,è¿™ä¸ªæ˜¯åŸºäºhdfså†…éƒ¨ç®¡ç†çš„ï¼Œå¦‚æœæŸäº›hdfsæ–‡ä»¶é«˜ioçš„tmpæ—¶æ–‡ä»¶ï¼Œæ˜¯ä¸€ä¸ªå¾ˆå¥½çš„é€‰æ‹©ã€‚  
å†…å­˜å­˜å‚¨ï¼šhttps://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/MemoryStorage.html
å®˜æ–¹æ–‡æ¡£ï¼šhttps://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/CentralizedCacheManagement.html


## é…ç½®ç›¸å…³
### å†…å­˜å­˜å‚¨
**å¿…é¡»é…ç½® æ­¤å‚æ•°å†³å®šäº† DataNode ç”¨äºç¼“å­˜çš„æœ€å¤§å†…å­˜é‡ï¼Œå­—èŠ‚ä¸ºå•ä½æŒ‡å®š.**     
`dfs.datanode.max.locked.memory=34359738368`  
**æŒ‚è½½ä¸€ä¸ª 32 GB çš„tmpfsåˆ†åŒºæˆ–è€…ç›´æ¥ç”¨ç³»ç»Ÿä¸Šå·²æŒ‚è½½çš„tmpåˆ†åŒº**    
`sudo mount -t tmpfs -o size=32g tmpfs /mnt/dn-tmpfs/`   
**æˆ–è€…(ç³»ç»Ÿä¸ä¸€æ ·ï¼ŒæŒ‚è½½çš„åå­—ä¹Ÿä¼šä¸ä¸€æ ·ï¼Œæ ¹æ®è‡ªå·±çš„ç³»ç»Ÿé€‰æ‹©)**       
```
df -h  
tmpfs            63G     0   63G   0% /dev/shm  
```
**tmpfsä½¿ç”¨ RAM_DISK å­˜å‚¨ç±»å‹æ ‡è®°å·**   
```xml
    <property>
      <name>dfs.datanode.data.dir</name>
      <value>/data1/x,/data1/x,/data3/x,[RAM_DISK]/dev/shm</value>
    </property>
    <!--å¦‚æœä¸ºç£ç›˜é…ç½®äº†reservedï¼Œè¿™ä¸ªéœ€è¦å•ç‹¬é…ç½®ï¼Œä¸ç„¶ä¼šé¢„ç•™å¾ˆå¤šç©ºé—´-->
    <property>
      <name>dfs.datanode.du.reserved.ram_disk</name>
      <value>0</value>
    </property>
```


### é›†ä¸­å¼ç¼“å­˜ç®¡ç†
**å¿…é¡»é…ç½® æ­¤å‚æ•°å†³å®šäº† DataNode ç”¨äºç¼“å­˜çš„æœ€å¤§å†…å­˜é‡ï¼Œå­—èŠ‚ä¸ºå•ä½æŒ‡å®š.**   
`dfs.datanode.max.locked.memory=34359738368`
//è¿™ä¸ªé…ç½®æ•°è¦ä½äºlinuxä¸Šçš„(ulimit -l)çš„å€¼ï¼Œè¿™ä¸ªå‚æ•°æ§åˆ¶è¿›ç¨‹å¯ä»¥å°†å¤šå°‘å†…å­˜é”å®šåœ¨ç‰©ç†RAMä¸­ï¼Œé˜²æ­¢è¢«äº¤æ¢åˆ°ç£ç›˜ã€‚ä¾‹ï¼š
`max locked memory       (kbytes, -l) 64`

é»˜è®¤å¤ªå°ï¼Œtmpä¿®æ”¹åˆ°32g  
`ulimit -l 33554432`
æ°¸ä¹…ä¿®æ”¹ ï¼ˆéœ€è¦rootæƒé™ï¼‰ ç¼–è¾‘   
`echo -e "\nhdfs soft memlock 33554432\nhdfs hard memlock 33554432" >> /etc/security/limits.d/hdfs.conf`
æŸ¥çœ‹è¿™ä¸ªå€¼  
`ulimit -l`


å¯åŠ¨æ—¥å¿—æ˜¾ç¤ºCannot start datanode because the configured max locked memory size é—®é¢˜   
```
2025-12-08 14:32:21,168 ERROR datanode.DataNode (DataNode.java:secureMain(2883)) - Exception in secureMain
java.lang.RuntimeException: Cannot start datanode because the configured max locked memory size (dfs.datanode.max.locked.memory) of 3612361255 bytes is more than the datanode's available RLIMIT_MEMLOCK ulimit of 65536 bytes.
        at org.apache.hadoop.hdfs.server.datanode.DataNode.startDataNode(DataNode.java:1389)
        at org.apache.hadoop.hdfs.server.datanode.DataNode.<init>(DataNode.java:500)
        at org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:2782)
        at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:2690)
        at org.apache.hadoop.hdfs.server.datanode.DataNode.createDataNode(DataNode.java:2732)
        at org.apache.hadoop.hdfs.server.datanode.DataNode.secureMain(DataNode.java:2876)
        at org.apache.hadoop.hdfs.server.datanode.SecureDataNodeStarter.start(SecureDataNodeStarter.java:100)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.commons.daemon.support.DaemonLoader.start(DaemonLoader.java:243)

hdfs@on-test-hadoop-65-239:/home/liangrui06$ cat /proc/34131/limits
Limit                     Soft Limit           Hard Limit           Units     
Max locked memory         65536                65536                bytes     
...    
```
**è§£å†³æ–¹æ¡ˆ**   

1ï¼šambari ä¸­çš„hadoop-env.shæ¨¡æ¿ä¹Ÿæœ‰ç‚¹é—®é¢˜, "$command" == "datanode" è¿™ä¸ªå˜é‡æˆ‘çš„ç¯å¢ƒæ˜¯ç©ºçš„ï¼Œéœ€è¦å»æ‰   
```
 # Fix temporary bug, when ulimit from conf files is not picked up, without full relogin.
 # Makes sense to fix only when runing DN as root
 #  if [ "$command" == "datanode" ] && [ "$EUID" -eq 0 ] && [ -n "$HDFS_DATANODE_SECURE_USER" ]; then
 if [ "$EUID" -eq 0 ] && [ -n "$HDFS_DATANODE_SECURE_USER" ]; then
 {% if is_datanode_max_locked_memory_set %}
 ulimit -l {{datanode_max_locked_memory}}
 {% endif %}
 ulimit -n {{hdfs_user_nofile_limit}}
 fi
```

2ï¼šåªä¿®æ”¹ä¸Šè€Œé‚£ä¸ªæ–‡ä»¶ä¸è¡Œï¼Œå‘ç°åœ¨ambariä¸­å¯åŠ¨çš„æ—¶å€™ï¼Œä¼šè¦†ç›–è¿™ä¸ªç©æ„ï¼ŒåŸæ¥æ˜¯ambariå†…éƒ¨æœ‰ä¸ªetcé…ç½®æ–‡ä»¶ï¼Œéœ€è¦æ‰‹åŠ¨ä¿®æ”¹ï¼Œåœ¨é¡µé¢ä¸Šæ‰¾ä¸åˆ°å…¥å£ä¿®æ”¹,å•ä½æ˜¯kbã€‚

`echo -e "\n{{hdfs_user}}   - memlock {{hdfs_user_memlock_limit|default(33554432)}}" >> /data/ambari-agent/cache/stacks/HDP/3.0/services/HDFS/package/templates/hdfs.conf.j2`  
æˆ–    
`echo -e "\n*   - memlock {{hdfs_user_memlock_limit|default(33554432)}}" >> /data/ambari-agent/cache/stacks/HDP/3.0/services/HDFS/package/templates/hdfs.conf.j2`    
ambari-serviceæ–‡ä»¶ä¹Ÿéœ€è¦æ›´æ”¹   
`echo -e "\n{{hdfs_user}}   - memlock {{hdfs_user_memlock_limit|default(33554432)}}" >> /var/lib/ambari-server/resources/stacks/HDP/3.0/services/HDFS/package/templates/hdfs.conf.j2`

è¿™ä¸ªé…ç½®æµ‹è¯•æ²¡æœ‰ç”¨åˆ°ï¼Œmarkä¸€ä¸‹       
`echo -e "\nhdfs soft memlock 33554432\nhdfs hard memlock 33554432" >> /usr/hdp/3.1.0.0-78/etc/security/limits.d/hdfs.conf`

3ï¼šå¦‚æœè¿˜ä¸è¡Œï¼Œå°±ç”¨é€šé…ç¬¦é…ç½®
```
echo "* - memlock 33554432" | sudo tee -a /etc/security/limits.conf
```

4:éªŒè¯è¿›ç¨‹çš„locked memory
```
cat /proc/${PID}$/limits | grep 'locked memory'
Max locked memory         35184372088832       35184372088832       bytes     
```
æ—¥å¿—æŸ¥çœ‹   
```
2025-12-10 15:29:27,346 INFO  impl.FsDatasetImpl (FsVolumeList.java:run(203)) - Time to add replicas to map for block pool BP-1514249846-10.12.65.19-1704289111087 on volume /dev/shm: 2ms
```


## cacheå‘½ä»¤
```
# æ–°å»ºpool å’Œè·¯å¾„
hdfs cacheadmin -addPool p001
hdfs cacheadmin -addDirective -path /cache/001 -pool p001 
hdfs cacheadmin -addDirective -path /cache/002  -pool p001  -replication 1 -ttl 1h
hdfs dfs -mkdir  /cache/001

#ä¿®æ”¹æƒé™  
hdfs cacheadmin -modifyPool p001 -mode 777

# æŸ¥çœ‹
hdfs cacheadmin -listDirectives 
Found 1 entry
 ID POOL   REPL EXPIRY  PATH       
  1 p001      1 never   /cache/001

hdfs cacheadmin -listDirectives  -stats  
hdfs cacheadmin -listPools -stats  

# é€šè¿‡listæŸ¥çœ‹id è¿›è¡Œåˆ é™¤
hdfs cacheadmin -removeDirective id  
hdfs cacheadmin -removeDirectives <path>

# æé«˜å†™çš„æ•ˆç‡ï¼Œå…ˆå†™å†…å­˜åå¼‚æ­¥åˆ·åˆ°ç£ç›˜ä¸­  è®¾ç½®ç›®å½•ä¸º LAZY_PERSIST ç­–ç•¥
hdfs storagepolicies -setStoragePolicy -path /cache/002 -policy LAZY_PERSIST

```


## éªŒè¯
### æŸ¥çœ‹ç›¸å…³æŒ‡æ ‡  
æ‰“å¼€æœåŠ¡èŠ‚ç‚¹åï¼Œåœ¨æ˜¾ç¤ºå­˜å‚¨ä¿¡æ¯æ—¶ï¼Œä¼šæœ‰å†…å­˜å­˜å‚¨ä¿¡æ¯   
![alt text](img/image-1.png)  

åœ¨cacheä¸­putä¸€ä¸ªæ–‡ä»¶  
```bash
hdfs dfs -put mysql-connector-java-5.1.49.jar /cache/002/
hdfs dfs -cat /cache/002/mysql-connector-java-5.1.49.jar > /dev/null 2>&1
```


æŸ¥çœ‹cacheä¸­çš„ç»Ÿè®¡   
```
hdfs cacheadmin -listDirectives -stats
Found 2 entries
 ID POOL   REPL EXPIRY                    PATH         BYTES_NEEDED  BYTES_CACHED  FILES_NEEDED  FILES_CACHED
  1 p001      1 never                     /cache/001        1006904       1006904             1             1
  2 p001      1 2025-12-09T16:02:01+0800  /cache/002        1006904       1006904             1             1
```

æŸ¥çœ‹èŠ‚ç‚¹Cache Used  
`hdfs dfsadmin -report`   
```
... 
Name: 10.12.65.x:1019 (on-test-hadoop-65-239.x.x.x.x.com)
Hostname: on-test-hadoop-65-239.hiido.host.int.yy.com
Rack: /4F08-06-04
Decommission Status : Normal
Configured Capacity: 41337038389248 (37.60 TB)
DFS Used: 1463606235771 (1.33 TB)
Non DFS Used: 0 (0 B)
DFS Remaining: 37472759803269 (34.08 TB)
DFS Used%: 3.54%
DFS Remaining%: 90.65%
Configured Cache Capacity: 34359738368 (32 GB)
Cache Used: 2015232 (1.92 MB)
Cache Remaining: 34357723136 (32.00 GB)
Cache Used%: 0.01%
Cache Remaining%: 99.99%
Xceivers: 2
Last contact: Tue Dec 09 15:08:33 CST 2025
Last Block Report: Tue Dec 09 14:51:33 CST 2025
Num of Blocks: 554506
```  
å¯ä»¥çœ‹åˆ° Configured Cache Capacity: 34359738368 (32 GB)  Cache Used: 2015232 (1.92 MB)  ç¬¦åˆé¢„æœŸ

### å¦‚ä½•ç¡®å®šè¯»çš„æ˜¯cache

dn.log  
```
2025-12-10 17:30:41,882 INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(742)) - DatanodeCommand action: DNA_CACHE for BP-1099381363-10.12.76.180-1704271160327 of [1082817566]
2025-12-10 17:31:41,888 INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(742)) - DatanodeCommand action: DNA_CACHE for BP-1099381363-10.12.76.180-1704271160327 of [1082817566]
```


disk  ç£ç›˜ä¸Šå¯ä»¥çœ‹åˆ°ç‰©ç†æ–‡ä»¶
```
root@on-test-hadoop-65-239:/home/liangrui06# ll  /data*/hadoop/hdfs/data/current/BP-1099381363-10.12.76.180-1704271160327/current/finalized/subdir*/subdir*/1082817566
-rw-r--r-- 1 hdfs hadoop 3949625 Dec 10 18:12 /data5/hadoop/hdfs/data/current/BP-1099381363-10.12.76.180-1704271160327/current/finalized/subdir10/subdir28/1082817566
```
RMA_DISK  å†…å­˜ä¸Šé¢çœ‹ä¸åˆ°ä»»ä½•ç‰©ç†æ–‡ä»¶ï¼Œè¿™æ˜¯æ­£å¸¸çš„
```
root@on-test-hadoop-65-239:/dev/shm/current# du -sh /dev/shm/current/BP*/current/finalized
0       /dev/shm/current/BP-1099381363-10.12.76.180-1704271160327/current/finalized
0       /dev/shm/current/BP-1514249846-10.12.65.19-1704289111087/current/finalized
```
HDFS çš„ç¼“å­˜ä¸æ˜¯æŠŠæ–‡ä»¶å¤åˆ¶åˆ° \/dev/shm`ï¼Œè€Œæ˜¯é€šè¿‡ mmap()+mlock()å°†æ–‡ä»¶é¡µé”åˆ°è¿›ç¨‹åœ°å€ç©ºé—´ï¼ˆå†…å­˜é¡µï¼‰ï¼Œå› æ­¤åœ¨`/dev/shm`` ä¸‹çœ‹ä¸åˆ°æ•°æ®æ˜¯æ­£å¸¸çš„ã€‚   
éªŒè¯æ˜¯å¦çœŸçš„è¢«ç¼“å­˜ï¼ˆé¡µé¢é©»ç•™åœ¨å†…å­˜ä¸­ï¼‰å¯ä»¥é€šè¿‡è¿›ç¨‹å†…å­˜æ˜ å°„/é¡µé¢é©»ç•™å·¥å…·æˆ– DataNode æš´éœ²çš„ç¼“å­˜æŒ‡æ ‡æ¥ç¡®è®¤ã€‚

```
root@on-test-hadoop-65-239:/dev/shm/current# pmap -x 13979 | grep blk_1082817581
00007f5f86971000    3860    3860       0 r--s- blk_1082817581
00007f5f86971000       0       0       0 r--s- blk_1082817581
root@on-test-hadoop-65-239:/dev/shm/current# grep blk_1082817581 /proc/13979/maps
7f5f86971000-7f5f86d36000 r--s 00000000 08:51 115343625                  /data5/hadoop/hdfs/data/current/BP-1099381363-10.12.76.180-1704271160327/current/finalized/subdir10/subdir28/blk_1082817581
root@on-test-hadoop-65-239:/dev/shm/current# grep -n blk_1082817581 /proc/13979/maps
12:7f5f86971000-7f5f86d36000 r--s 00000000 08:51 115343625                  /data5/hadoop/hdfs/data/current/BP-1099381363-10.12.76.180-1704271160327/current/finalized/subdir10/subdir28/blk_1082817581
```    

ä»æ—¥å¿—ä¸­æŸ¥çœ‹cacheä¿¡æ¯,å¼€å¯debug  
`hadoop  daemonlog -setlevel on-test-hadoop-65-239.hiido.host.int.yy.com:1022 org.apache.hadoop.hdfs.server.datanode debug`    
æŸ¥çœ‹æ—¥å¿—ä¿¡æ¯æ˜¾ç¤ºSuccessfully cached è¯´æ˜cacheæˆåŠŸ  
```
root@on-test-hadoop-65-239:/data/logs/hadoop/hdfs# grep 1082817943  hadoop-hdfs-root-datanode-on-test-hadoop-65-239.hiido.host.int.yy.com.log  | grep cache
2025-12-11 11:28:58,286 DEBUG impl.FsDatasetCache (FsDatasetCache.java:cacheBlock(309)) - Initiating caching for Block with id 1082817943, pool BP-1099381363-10.12.76.180-1704271160327
2025-12-11 11:28:58,291 DEBUG impl.FsDatasetCache (FsDatasetCache.java:run(486)) - Successfully cached 1082817943_BP-1099381363-10.12.76.180-1704271160327.  We are now caching 8912896 bytes in total.
2025-12-11 11:29:58,285 DEBUG impl.FsDatasetCache (FsDatasetCache.java:cacheBlock(300)) - Block with id 1082817943, pool BP-1099381363-10.12.76.180-1704271160327 already exists in the FsDatasetCache with state CACHED
```

## ç›‘æ§ 

| Name                   | Description                                                         |
|------------------------|---------------------------------------------------------------------|
| BlocksCached           | Total number of blocks cached                                       |
| BlocksUncached         | Total number of blocks uncached                                     |
| CacheReportsNumOps     | Total number of cache report operations                             |
| CacheReportsAvgTime    | Average time of cache report operations in milliseconds             |

grafnaé…ç½®æ•ˆæœ  
![alt text](img/image-2.png)


<div class="post-date">
  <span class="calendar-icon">ğŸ“…</span>
  <span class="date-label">å‘å¸ƒï¼š</span>
  <time datetime="2025-12-10" class="date-value">2025-12-10</time>
</div>

<div class="outline" style="background:#f6f8fa;padding:1em 1.5em 1em 1.5em;margin-bottom:1em;border-radius:8px;">
  <strong>å¤§çº²ï¼š</strong>
  <ul id="outline-list" style="margin:0;padding-left:1.2em;"></ul>
</div>

<!--èœå•æ -->
  <nav class="blog-nav">
    <button class="collapse-btn" onclick="toggleBlogNav()">â˜°</button>
    {% include blog_navigation.html items=site.data.blog_navigation %}
 </nav>

 <script src="/assets/blog.js"></script>
<link rel="stylesheet" href="/assets/blog.css">