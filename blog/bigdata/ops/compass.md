---
layout: default
title:  compass
description: "OPPO å¤§æ•°æ®è¯Šæ–­å¹³å° ç½—ç›˜" 
keywords: compass,ç½—ç›˜,OPPO å¤§æ•°æ®è¯Šæ–­å¹³å°,OPPO compass
author: liangrui
date: 2025-08-15
---

<div class="post-date">
  <span class="calendar-icon">ğŸ“…</span>
  <span class="date-label">å‘å¸ƒï¼š</span>
  <time datetime="2025-08-18" class="date-value">2025-08-15</time>
</div>

<script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
<script>
  mermaid.initialize({ startOnLoad: true });
</script>

<div class="outline" style="background:#f6f8fa;padding:1em 1.5em 1em 1.5em;margin-bottom:2em;border-radius:8px;">
  <strong>å¤§çº²ï¼š</strong>
  <ul id="outline-list" style="margin:0;padding-left:1.2em;"></ul>
</div>


# compassè¯Šæ–­å¹³å°æ¨¡å—åˆ†æ
åº”ç”¨æ–‡æ¡£å¯ä»¥ç›´æ¥æŸ¥çœ‹ï¼š<a href="https://github.com/cubefs/compass "> compassGithub</a>
å› :è°ƒåº¦ç³»ç»Ÿæ˜¯è‡ªç ”çš„ï¼Œmysqlå¹³å°ä¸æ”¯æŒcanalé‡‡é›†,è¿™é‡Œå¯¹æºç åšäº†åˆ†æï¼Œè¿›è¡Œäº†æ•°æ®å¯¹æ¥å’Œè½¬æ¢


<div class="mermaid">
flowchart TD
  A[canal<br>åŒæ­¥è°ƒåº¦æ•°æ®è¡¨åˆ°compassè¡¨] --> B[task syncer<br>æ¶ˆè´¹mysqldataè½¬å­˜ä¸ºcompassè¡¨<br>å†™kafka: task-instance]
  B --> C[task application<br>æ¶ˆè´¹task-instance<br>æ—¥å¿—æå–app_id<br>å†™mysql: task_application<br>å‘kafka: task-application]
  C --> D[flinkæ¨¡å—<br>æ¶ˆè´¹task-application]
  B --> E[task-detect<br>æ¶ˆè´¹task-instance<br>å¼‚å¸¸æ£€æµ‹<br>å†™ES: compass-job-instance<br>å†™Redis: delayed:task]
  C --> F[task parser<br>æ¶ˆè´¹Redis: parser:processing<br>å¼•æ“å±‚å¼‚å¸¸æ£€æµ‹]
  E --> G[task portal<br>å‰ç«¯æ¥å£<br>æŠ¥å‘Šæ€»è§ˆ/è°ƒåº¦åˆ—è¡¨/ç¦»çº¿åˆ—è¡¨/è¯Šæ–­]
  F --> G
  C --> G

  subgraph DolphinScheduler
    H[flow<br>å·¥ä½œæµå®šä¹‰è¡¨]
    I[task<br>ä»»åŠ¡å®šä¹‰è¡¨]
    J[task_instance<br>ä»»åŠ¡å®ä¾‹è¡¨]
    H --> I
    I --> J
  end
</div>

## compassæœåŠ¡æ¨¡å—ä½œç”¨
- **canalä½œç”¨**  
é€šè¿‡kafka ä¸»é¢˜ä¸º:mysqldata, è¿›è¡ŒåŒæ­¥è°ƒåº¦æ•°æ®è¡¨åˆ°compassè¡¨  
adapterä¸»è¦æ˜¯é€‚é…ä¸åŒè°ƒåº¦è¡¨æ•°æ®ï¼Œä¸»è¦é…ç½®srcDataSources:æºè°ƒåº¦çš„æ•°æ®æºï¼Œ canalAdapters:ç›®æ ‡æ•°æ®æº
å…·ä½“è¡¨è½¬æ¢è§„åˆ™åœ¨ï¼štask-canal-adapter/src/main/adapter/conf/rdb/xx.yml è¿›è¡Œé…ç½®
- **task metadata**  
ä¸»è¦æ˜¯åŒæ­¥spark yarn çš„ä½œä¸šå…ƒæ•°æ®
- **task syncer**  
é€šè¿‡æ¶ˆè´¹ kafkaä¸»é¢˜:mysqldataçš„è°ƒåº¦mysqlè¡¨æ•°æ®ï¼Œè½¬å­˜ä¸ºcompassè¡¨ 
å¹¶ä¸”å†™kafkaå…¥ä¿¡æ¯(xx_task_instanceè¡¨)ï¼šæ¶ˆè´¹topicï¼šmysqldata  ->  å‘é€ topic:task-instance

- **task application**  
å°†å·¥ä½œæµå±‚ä¸å¼•æ“å±‚å…ƒæ•°æ®å…³è”  
æ¶ˆè´¹ kafkaä¸»é¢˜:task-instance, é€šè¿‡task_instance.idä»task_instanceè¡¨ä¸­æŸ¥è¯¢å‡ºå®ä¾‹ä¿¡æ¯  
é€šè¿‡è§£ææ—¥å¿—æ–‡ä»¶ï¼Œä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…æå–å‡ºæ¥æ‰¾åˆ°å¯¹åº”çš„application_id,->rules.extractLog.name
è½¬æ¢æ•°æ®åå†™å…¥mysql -> compass.task_applicationè¡¨ä¸­ï¼Œ
å¹¶å°†TaskApplicationä¿¡æ¯å‘é€åˆ°kafka -> ä¸»é¢˜ä¸º:task-application ->flinkæ¨¡å—è¿›è¡Œæ¶ˆè´¹

- **task-detect**  
æ¨¡å—è¿›è¡Œå·¥ä½œæµå±‚å¼‚å¸¸ä»»åŠ¡æ£€æµ‹ï¼Œä¾‹å¦‚è¿è¡Œå¤±è´¥ã€åŸºçº¿è€—æ—¶å¼‚å¸¸ç­‰  
DetectedTaské€šè¿‡æ¶ˆè´¹kafkaä¸»é¢˜ï¼štask-instanceè¿›è¡Œå¤„ç†é€»è¾‘,å–åˆ°çš„æ˜¯taskå®ä¾‹ä¿¡æ¯ï¼Œ  
å†é€šè¿‡projectName,flowName,taskName,executionTimeå»mysqlè¡¨ï¼štask_applicationæŸ¥è¯¢å‡ºappä¿¡æ¯ï¼Œ  
ç„¶åæŠŠè¯Šæ–­ç»“æœå†™å…¥  
ES -> (compass-job-instance)   
å»¶è¿Ÿçš„å®ä¾‹å¤„ç†ï¼ˆç¼ºå°‘appidï¼‰   
Redis -> ({lua}:delayed:task)    
DelayedTaskå»¶è¿Ÿä»»åŠ¡å¤„ç†ï¼Œé€šè¿‡spingBoot->CommandLineRunnerå®ç°å¯åŠ¨æ—¶è¿è¡Œ  


- **task parser**  
è¿›è¡Œå¼•æ“å±‚å¼‚å¸¸ä»»åŠ¡æ£€æµ‹ï¼Œä¾‹å¦‚SQLå¤±è´¥ã€Shuffleå¤±è´¥ç­‰ 
ä»redisä¸­æ¶ˆè´¹->{lua}:parser:processing 

- **task portal**  
å‰ç«¯é¡µé¢å±•ç¤ºç›¸å…³æ¥å£æ¨¡å—
æŠ¥å‘Šæ€»è§ˆ      ReportController -> /api/v1/report  
è°ƒåº¦åˆ—è¡¨å…¥å£ä¸º AppController -> /api/v1/app/list  -> æŸ¥è¯¢ESç´¢å¼•compass-task-app*   
ç¦»çº¿åˆ—è¡¨å…¥å£ä¸º JobController -> /api/v1/job/list  -> æŸ¥è¯¢ESç´¢ä¸Šcompass-job-analysis*  
ç¦»çº¿è¯Šæ–­å…¥å£  /openapi/offline/app/metadata -> redis:{lua}:log:record ->| task-parser -> RedisConsumeræ•°æ®æ¶ˆè´¹  redis:{lua}:log:record 

- **å¼‚å¸¸ç»Ÿè®¡ES**  
  - å¼‚å¸¸åˆ—è¡¨ï¼šJobController -> /jobDiagnoseInfo -> ESç´¢å¼•:compass-detector-${date}->æ•°æ®æ ¼å¼:DetectorStorage->BigTableScanService å¤„ç†åç”Ÿæˆ->å±•ç¤ºä½œä¸šå¼‚å¸¸ä¿¡æ¯


## dolphinSchedulerä¸»è¦è¡¨å…³ç³»
flow è¡¨ï¼ˆå·¥ä½œæµå®šä¹‰è¡¨ï¼‰  
task è¡¨ï¼ˆä»»åŠ¡å®šä¹‰è¡¨ï¼‰  
task_instance è¡¨ï¼ˆä»»åŠ¡å®ä¾‹è¡¨ï¼‰  

### ä¸‰è€…çš„å…³ç³»   
#### å±‚çº§å…³ç³»ï¼š
  ä¸€ä¸ª flowï¼ˆå·¥ä½œæµï¼‰åŒ…å«å¤šä¸ª taskï¼ˆä»»åŠ¡èŠ‚ç‚¹ï¼‰  
  å½“å·¥ä½œæµè¢«æ‰§è¡Œæ—¶ï¼Œä¼šç”Ÿæˆå·¥ä½œæµå®ä¾‹ï¼ŒåŒæ—¶ä¸ºæ¯ä¸ªä»»åŠ¡èŠ‚ç‚¹ç”Ÿæˆ task_instance  
#### æ•°æ®æµå‘ï¼š
  ç”¨æˆ·å…ˆå®šä¹‰ flowï¼ˆå·¥ä½œæµï¼‰  
  åœ¨ flow ä¸­æ·»åŠ å¤šä¸ª taskï¼ˆä»»åŠ¡èŠ‚ç‚¹ï¼‰å¹¶è®¾ç½®ä¾èµ–å…³ç³»  
  è°ƒåº¦æˆ–æ‰‹åŠ¨è§¦å‘æ—¶ï¼Œç³»ç»Ÿæ ¹æ® flow å’Œ task å®šä¹‰ç”Ÿæˆ task_instance æ‰§è¡Œ  
#### ç”Ÿå‘½å‘¨æœŸï¼š
  flow å’Œ task æ˜¯é™æ€å®šä¹‰ï¼Œä¸€èˆ¬ä¸éšæ‰§è¡Œæ”¹å˜  
  task_instance æ˜¯åŠ¨æ€ç”Ÿæˆçš„ï¼Œæ¯æ¬¡æ‰§è¡Œéƒ½ä¼šåˆ›å»ºæ–°è®°å½•  


# è‡ªå®šä¹‰è°ƒåº¦ç³»ç»Ÿè¡¨è½¬æ¢æµç¨‹
è¿™é‡Œé‡‡ç”¨äº†sparkæ¯å°æ—¶é‡‡é›†è‡ªå®šä¹‰è°ƒåº¦ç³»ç»Ÿå’Œkyuubiè¡¨ä¿¡æ¯ï¼Œæ‰¾åˆ°è°ƒåº¦->å®ä¾‹->applicationç›¸å…³ä¿¡æ¯ï¼Œæ‰¹é‡æ¸…æ´—å®Œæ•°æ®åï¼Œç»Ÿä¸€æ‰¹æ¬¡å‘é€åˆ°kafka,  
è¯Šæ–­ç³»ç»Ÿï¼ˆtask-detectï¼‰ä¼šæ¶ˆè´¹kafkaæ¶ˆæ¯ï¼Œæ¥è¿›è¡Œè‡ªåŠ¨è¯Šæ–­ï¼ŒæŠŠç»“æœå­˜å…¥ESè¿›è¡Œå±•ç¤ºï¼Œè¿™é‡Œå°±ç›´æ¥è·³è¿‡äº†task-canalå’Œtask-applicaioné¡¹ç›®å¤„ç†çš„é€»è¾‘ã€‚
 

<div class="mermaid">
graph TD
  A[æºMySQLæ•°æ®åº“] -->|1. è¯»å–æ•°æ®| B[Spark Session]
  C[Kyuubi MySQL] -->|2. è¯»å–åº”ç”¨ID| B
  B -->|3. æ•°æ®è½¬æ¢| D[ä¸´æ—¶DataFrame]
  D -->|4.1 å†™å…¥ç›®æ ‡MySQL| E[ç›®æ ‡MySQL task_instanceè¡¨]
  D -->|4.2 å†™å…¥ç›®æ ‡MySQL| F[ç›®æ ‡MySQL task_applicationè¡¨]
  D -->|5. è¿‡æ»¤æœ‰app_idçš„æ•°æ®| G[Kafkaç”Ÿäº§æ•°æ®]
  G -->|6. å‘é€æ¶ˆæ¯| H[Kafkaä¸»é¢˜ task-instance]
</div>
  

# æµç¨‹æ­¥éª¤è¯´æ˜

## æ•°æ®æºè¯»å–
###  æºMySQLï¼š
è¡¨ï¼šJOB_INST_1ï¼ˆå®ä¾‹ä¿¡æ¯ï¼‰ã€JOB_DESCï¼ˆä»»åŠ¡æè¿°ï¼‰ã€HOST_GROUP_DEFï¼ˆä¸»æœºç»„:è°ƒåº¦æ²¡æœ‰flowæ¦‚å¿µï¼Œåªæœ‰ä¾èµ–æ‹“æ‰‘å…³ç³»ï¼Œå æ—¶ç”¨è¿™ä¸ªä»£æ›¿ï¼Œåé¢å†æ¸…å…ˆè½¬æ¢ï¼‰  
 SQLæ¡ä»¶ï¼šç­›é€‰æŒ‡å®šæ—¶é—´èŒƒå›´ï¼ˆdateStartHouråˆ°å½“æ—¥23:59:59ï¼‰ä¸”ä»»åŠ¡ç±»å‹ä¸º99æˆ–96çš„è®°å½•ã€‚  

### Kyuubi MySQLï¼š
è¡¨ï¼šsqlmetadata   è·å–Sparkä»»åŠ¡çš„application_idï¼ŒæŒ‰æ—¶é—´èŒƒå›´è¿‡æ»¤å¹¶å»é‡ã€‚  

## æ•°æ®è½¬æ¢

### UDFå¤„ç†ï¼š
getTaskTypeï¼šå°†æ•°å­—ä»»åŠ¡ç±»å‹è½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼ˆå¦‚99â†’SPARKï¼‰ã€‚  
getTaskStateï¼šå°†çŠ¶æ€ç è½¬ä¸ºæ–‡æœ¬ï¼ˆå¦‚1â†’successï¼‰ã€‚  
getTriggerTypeï¼šåŒºåˆ†è§¦å‘ç±»å‹ï¼ˆscheduleæˆ–manualï¼‰ã€‚  
getExecutionTimeï¼šè§„èŒƒåŒ–æ—¶é—´æ ¼å¼ã€‚  
### å­—æ®µæ˜ å°„ï¼š
#### å­—æ®µæ˜ å°„é€»è¾‘ï¼šå‚è€ƒä»£ç æ³¨é‡Šä¸­çš„æµ·çŒ«ï¼ˆæºç³»ç»Ÿï¼‰ä¸Compassï¼ˆç›®æ ‡ç³»ç»Ÿï¼‰å­—æ®µå¯¹åº”å…³ç³»ã€‚
æºå­—æ®µï¼ˆå¦‚BUSI_GRPï¼‰â†’ç›®æ ‡å­—æ®µï¼ˆå¦‚project_nameï¼‰...ç­‰
æ•°æ®å†™å…¥ç›®æ ‡MySQL
task_instanceè¡¨ï¼š
ä½¿ç”¨ä¸´æ—¶è¡¨+ON DUPLICATE KEY UPDATEå®ç°å¹‚ç­‰å†™å…¥ï¼ˆæŒ‰idæ›´æ–°ï¼‰ã€‚
task_applicationè¡¨ï¼š
å…³è”å®ä¾‹æ•°æ®ä¸application_idï¼Œå†™å…¥ä»»åŠ¡åº”ç”¨ä¿¡æ¯ã€‚  
...
## Kafkaæ¶ˆæ¯ç”Ÿäº§
æ•°æ®è¿‡æ»¤ï¼šä»…é€‰æ‹©åŒ…å«application_idçš„å®ä¾‹è®°å½•ã€‚
### æ¶ˆæ¯æ ¼å¼ï¼š    
```
{
  "rawData": null,
  "body": {
    "id": "å®ä¾‹ID",
    "projectName": "é¡¹ç›®å",
    "flowName": "æµåç§°",
    ...
  },
  "eventType": "INSERT",
  "table": "task_instance"
}
```
### æœ€ç»ˆæ•ˆæœé¢„è§ˆ
![alt text](../../../image/ops/compass/01.png)

## è¯Šæ–­é€»è¾‘è§£æ

 é»˜è®¤è¯Šæ–­é…ç½®:compass\task-parser\src\main\resources\application.yml  

å›¾è§£æµç¨‹(sparkä¸­çš„job->stage->taskçš„æµç¨‹)
å‡è®¾æˆ‘ä»¬æœ‰å¦‚ä¸‹ä»£ç ï¼š  

```scala
//RDD is resilient distributed dataset
val textFile = sc.textFile("hdfs://...")          // RDD A
val words = textFile.flatMap(line => line.split(" ")) // RDD B
val mappedWords = words.map(word => (word, 1))       // RDD C
val wordCounts = mappedWords.reduceByKey(_ + _)     // RDD D
wordCounts.saveAsTextFile("hdfs://...output")      // Action!
```
å…¶æ‰§è¡Œæµç¨‹çš„ DAG å’Œ Stage åˆ’åˆ†å¦‚ä¸‹å›¾æ‰€ç¤º  

<div class="mermaid">
flowchart TD
    subgraph Application [Application - åº”ç”¨ç¨‹åº]
        direction TB
        A[textFile RDD A] --> B[flatMap RDD B]
        B --> C[map RDD C]
        C -- Wide Dependency<br>Shuffle! --> D[reduceByKey RDD D]
        D --> Action[saveAsTextFile Action]
    end

    Application --> Job

    subgraph Job [Job - ä½œä¸š]
        direction TB
        S1[Stage 1<br>ShuffleMapStage] -- Shuffle Data --> S2[Stage 2<br>ResultStage]
    end

    Job --> Stage

    subgraph Stage1 [Stage 1 å†…éƒ¨]
        direction LR
        T1_1[Task 1]
        T1_2[Task 2]
        T1_3[...]
        T1_4[Task N]
    end

    subgraph Stage2 [Stage 2 å†…éƒ¨]
        direction LR
        T2_1[Task 1]
        T2_2[Task 2]
        T2_3[...]
        T2_4[Task M]
    end

    Stage --> Task

    T1_1 --> E1[Executor Core]
    T1_2 --> E2[Executor Core]
    T1_4 --> E3[Executor Core]
</div>

**æµç¨‹è§£é‡Š (å¯¹åº”ä¸Šå›¾æ•°å­—)ï¼š**
- æ‰§è¡Œ saveAsTextFile (Action)ï¼Œè§¦å‘ä¸€ä¸ª Jobã€‚  
- DAGScheduler ä» RDD D åå‘å›æº¯ï¼Œå‘ç° reduceByKey æ˜¯ä¸€ä¸ªå®½ä¾èµ–ã€‚  
- åœ¨å®½ä¾èµ–å¤„åˆ’å¼€ï¼ŒreduceByKey ä¹‹åçš„æ“ä½œï¼ˆæœ¬ä¾‹ä¸­æ²¡æœ‰ï¼‰å±äº Stage 2 (Result Stage)ï¼Œä¹‹å‰çš„æ‰€æœ‰æ“ä½œ (textFile, flatMap, map) å±äº Stage 1 (Shuffle Map Stage)ã€‚
- Stage 1 å¯åŠ¨ã€‚å‡è®¾æºæ–‡ä»¶è¢«åˆ’åˆ†ä¸º N ä¸ªåˆ†åŒºï¼Œåˆ™ Stage 1 ä¼šåˆ›å»º N ä¸ª ShuffleMapTaskã€‚è¿™äº› Task è¢«åˆ†é…åˆ°å„ä¸ª Executor ä¸Šï¼Œè¯»å– HDFS æ•°æ®å—ï¼Œæ‰§è¡Œ flatMap å’Œ map æ“ä½œï¼Œç„¶åä¸º reduceByKey åšå‡†å¤‡ï¼ˆå¯¹- æ•°æ®è¿›è¡Œåˆ†åŒºå’Œæœ¬åœ°èšåˆï¼‰ï¼Œæœ€åå°†ç»“æœå†™å…¥æœ¬åœ°ç£ç›˜ï¼ˆShuffle æ–‡ä»¶ï¼‰ã€‚
- Stage 1 å…¨éƒ¨æ‰§è¡Œå®Œæ¯•åï¼ŒStage 2 å¯åŠ¨ã€‚reduceByKey é»˜è®¤çš„åˆ†åŒºå™¨ï¼ˆé€šå¸¸æ˜¯ HashPartitionerï¼‰ä¼šäº§ç”Ÿ M ä¸ªåˆ†åŒºï¼ˆé»˜è®¤å’Œçˆ¶ RDD åˆ†åŒºæ•°ä¸€è‡´ï¼‰ï¼Œæ‰€ä»¥ Stage 2 ä¼šåˆ›å»º M ä¸ª ResultTaskã€‚
- è¿™äº› ResultTask ä¼šå»æ‹‰å– (Fetch) Stage 1 ä¸­è¾“å‡ºçš„ã€å±äºè‡ªå·±åˆ†åŒºçš„ Shuffle æ•°æ®ï¼Œç„¶ååœ¨ Executor ä¸Šæ‰§è¡Œæœ€ç»ˆçš„èšåˆï¼ˆ_ + _ï¼‰æ“ä½œï¼Œæœ€åå°†ç»“æœä¿å­˜åˆ° HDFSã€‚

**æ€»ç»“**

 æ¦‚å¿µ   | äº§ç”Ÿæ–¹å¼             | æ•°é‡                      | è§„åˆ’è€…         | æ‰§è¡Œè€…
 ------ | -------------------- | ------------------------- | -------------- | ---------------
 Job    | ä¸€ä¸ª Action ç®—å­     | 1ä¸ªApplicationåŒ…å«å¤šä¸ªJob | Driver         | (æ•´ä½“)
 Stage  | æ ¹æ® å®½ä¾èµ– åˆ’åˆ†      | 1ä¸ªJobåŒ…å«å¤šä¸ªStage       | DAGScheduler   | (é˜¶æ®µ)
 Task   | ä¸ RDDåˆ†åŒº ä¸€ä¸€å¯¹åº”   | 1ä¸ªStageåŒ…å«å¤šä¸ªTask      | TaskScheduler  | Executor  

- **Taskï¼š** ä¸€ä¸ª Stage ä¼šæ ¹æ®å…¶åˆ†åŒºæ•°ï¼ˆPartitionsï¼‰è¢«æ‹†åˆ†æˆå¤šä¸ª Taskã€‚Task æ˜¯ Spark ä¸­æœ€åŸºæœ¬çš„å·¥ä½œå•å…ƒå’Œæ‰§è¡Œå•å…ƒï¼Œæ¯ä¸ª Task åœ¨ä¸€ä¸ª Executor çš„ä¸€ä¸ªæ ¸å¿ƒä¸Šå¤„ç†ä¸€ä¸ªåˆ†åŒºçš„æ•°æ®ã€‚ä¸€ä¸ª Stage çš„æ‰€æœ‰ Task æ‰§è¡Œçš„è®¡ç®—é€»è¾‘æ˜¯å®Œå…¨ä¸€æ ·çš„ï¼Œåªæ˜¯å¤„ç†çš„æ•°æ®ä¸åŒã€‚  
- stageä¸­å­˜åœ¨taskæœ€å¤§è¿è¡Œè€—æ—¶è¿œå¤§äºä¸­ä½æ•°çš„ä»»åŠ¡ä¸ºå¼‚å¸¸

## è¯Šæ–­é€»è¾‘æ¡ˆä¾‹åˆ†æ

### cpuæµªè´¹è®¡ç®—
#### executorè®¡ç®—
- ä»»åŠ¡å®é™…ä½¿ç”¨çš„è®¡ç®—èµ„æºï¼ˆæ¯«ç§’ï¼‰
  sparkæ‰€æœ‰çš„job æ‰§è¡Œæ—¶é—´ç›¸åŠ :  inJobComputeMillisUsed= (for-> spark.job.executorRunTime++)

- ä»»åŠ¡å¯ç”¨çš„è®¡ç®—èµ„æºï¼ˆæ¯«ç§’ï¼‰
  totalCores=executorCores*maxExecutorsï¼ˆæœ€å¤§å¹¶å‘executoræ•°ï¼‰
  inJobComputeMillisAvailable = totalCores * jobTime;

- cpuæµªè´¹æ¯”ä¾‹  
  executorWastedPercentOverAll = (inJobComputeMillisAvailable - inJobComputeMillisUsed) / inJobComputeMillisAvailable * 100%
- åˆ¤æ–­æ˜¯å¦æµªè´¹
  if (cpuæµªè´¹æ¯”ä¾‹45% < é˜ˆå€¼50%)=> æ­£å¸¸
**å¤‡æ³¨:**è¿™é‡Œå¦‚æœå¯ç”¨äº†spark åŠ¨æ€åˆ†é…è®¾ç½®(spark.dynamicAllocation.enabled)ï¼Œè®¡ç®—å®Œçš„executorä¼šå…³é—­ï¼Œå®‰è¿™ç§æ–¹å¼è®¡ç®—ï¼Œä¼šæŠŠå…³é—­åçš„executorä¹Ÿä¼šç®—ä¸ºåœ¨åº”ç”¨cpuï¼Œ è¿™æ ·çš„è¯è®¡ç®—æ˜¯ä¸åˆç†çš„

#### driverè®¡ç®—
- ä¸»è¦æ˜¯è®¡ç®— dirverä¸­é—´å¡é¡¿æ²¡æœ‰è®¡ç®—çš„æ¯”ä¾‹ï¼Œæ¯”å¦‚è°ƒåº¦ä¸‹ä¸€ä¸ªjobæ—¶æ²¡æœ‰èµ„æºå¯ç”¨
- appTotalTime è¡¨ç¤ºæ•´ä¸ªSparkåº”ç”¨çš„æ€»è¿è¡Œæ—¶é—´   
- jobTime è¡¨ç¤ºæ‰€æœ‰Sparkä½œä¸šå®é™…æ‰§è¡Œæ—¶é—´çš„æ€»å’Œ   
 driverComputeMillisWastedJobBased = driverTimeJobBased * totalCores  
 driverTimeJobBased = appTotalTime - jobTime ï¼ˆåº”ç”¨æ€»æ—¶é—´å‡å»ä½œä¸šæ—¶é—´ï¼‰  
 appComputeMillisAvailable = totalCores * appTotalTime ï¼ˆæ€»æ ¸å¿ƒæ•°ä¹˜ä»¥åº”ç”¨æ€»æ—¶é—´ï¼‰  
 ##### driver cpuæµªè´¹æ¯”ä¾‹
 driverWastedPercentOverAll =
                ((float) driverComputeMillisWastedJobBased / appComputeMillisAvailable) * 100;
åœ¨Sparkåº”ç”¨ä¸­ï¼Œ appTotalTime å’Œ jobTime å·®è·è¾ƒå¤§çš„æƒ…å†µä¸»è¦æœ‰ä»¥ä¸‹å‡ ç§ï¼š

#### appTotalTimeå’ŒjobTimeçš„å·®è·åŒºåˆ« è¯´æ˜
1. èµ„æºç­‰å¾… ï¼š     
   - å¯åŠ¨DriveråYARNæ²¡æœ‰å¯ç”¨èµ„æºæ—¶
   - ä½œä¸šæ‰§è¡Œè¿‡ç¨‹ä¸­èµ„æºè¢«æŠ¢å æˆ–é‡Šæ”¾åé‡æ–°ç”³è¯·
2. ä½œä¸šé—´éš”æœŸ ï¼š   
   - å½“ä¸€ä¸ªä½œä¸šå®Œæˆåˆ°ä¸‹ä¸€ä¸ªä½œä¸šå¼€å§‹æäº¤ä¹‹é—´çš„é—´éš”æ—¶é—´
   - è¿™ä¸ªé—´éš”æœŸä¼šè®¡å…¥ appTotalTime ä½†ä¸ä¼šè®¡å…¥ jobTime
3. å…¶ä»–æƒ…å†µ ï¼š   
   - Driveråˆå§‹åŒ–æ—¶é—´ï¼ˆåŠ è½½ä¾èµ–ã€æ³¨å†Œåº”ç”¨ç­‰ï¼‰
   - ä½œä¸šè°ƒåº¦å»¶è¿Ÿï¼ˆç‰¹åˆ«æ˜¯åœ¨åŠ¨æ€èµ„æºåˆ†é…æ¨¡å¼ä¸‹ï¼‰
   - æ•°æ®å€¾æ–œå¯¼è‡´çš„æŸäº›ä»»åŠ¡é•¿æ—¶é—´è¿è¡Œï¼Œè€Œå…¶ä»–èµ„æºå¤„äºç©ºé—²çŠ¶æ€  
   
##### æˆ‘ä»¬å½“å‰çš„ç¯å¢ƒ
- æˆ‘ä»¬ç›®å‰æ²¡æœ‰å¯ç”¨ä¸¥æ ¼cpuåˆ†é…å’Œé™åˆ¶
- å¯ç”¨saprkåŠ¨æ€åˆ†é…åå’Œè®¡ç®—é€»è¾‘å†²çª 
- spark kyuubiæœºå™¨å°±æ˜¯å­˜åœ¨æµªè´¹cpuå’Œå†…å­˜å¸¸é©»è¿›ç¨‹æœºå™¨æ¥æ¢å–åŠ é€Ÿå¯åŠ¨è¿›ç¨‹ï¼Œä¼šå­˜åœ¨æµªè´¹æƒ…å†µ 

**ç»¼åˆä»¥ä¸Šè€ƒè™‘ï¼Œè¿™ä¸ªè¯Šæ–­å¯¹æˆ‘ä»¬ç›®å‰ä¸é€‚ç”¨ï¼Œè°ƒå¤§è¿™äººé˜ˆå€¼åˆ°95**  
- executorThreshold: 95   
- driverThreshold: 95  


### Taské•¿å°¾ 
 **è¯Šæ–­æè¿°** ï¼šmap/reduce taskæœ€å¤§è¿è¡Œè€—æ—¶è¿œå¤§äºä¸­ä½æ•°çš„ä»»åŠ¡  
#### è®¡ç®—æ–¹å¼
```java
// è®¡ç®—æ¯ä¸ªtaskçš„æœ€å¤§æ‰§è¡Œæ—¶é—´ä¸ä¸­ä½æ•°çš„æ¯”å€¼
ratio = max_duration / median_duration
// taskDurationConfig.threshold default:10
å½“ ratio > threshold æ—¶ï¼ˆthresholdæ¥è‡ªé…ç½®ï¼‰ï¼Œåˆ¤å®šä¸ºé•¿å°¾å¼‚å¸¸
```
#### å»ºè®®ä¼˜åŒ–
##### é¦–å…ˆç¡®è®¤æ˜¯æ•°æ®å€¾æ–œè¿˜æ˜¯è®¡ç®—å€¾æ–œ
- å¦‚æœæŸä¸ª Task çš„ Shuffle Read æ•°æ®é‡è¿œå¤§äºå…¶ä»– Taskï¼ŒåŸºæœ¬å¯ä»¥æ–­å®šæ˜¯æ•°æ®å€¾æ–œã€‚å¦‚æœå¤„ç†çš„æ•°æ®é‡å·®ä¸å¤šï¼Œä½†æ‰§è¡Œæ—¶é—´å·®åˆ«å¤§ï¼Œå¯èƒ½æ˜¯è®¡ç®—å€¾æ–œï¼ˆä¾‹å¦‚æŸä¸ªåˆ†åŒºçš„æ•°æ®å¯¼è‡´äº†æ›´å¤æ‚çš„è®¡ç®—é€»è¾‘ï¼Œå¦‚æ·±å±‚å¾ªç¯ï¼‰ã€‚
##### ä¼˜åŒ–æ–¹å‘ä¸€ï¼šåº”å¯¹æ•°æ®å€¾æ–œ (Data Skewness)
 **è¿™æ˜¯æœ€å¸¸è§çš„åŸå› ï¼Œå³æŸäº› Key å¯¹åº”çš„æ•°æ®é‡è¿œå¤§äºå…¶ä»– Keyã€‚**
 **a) é¢„å¤„ç†æ•°æ®æº**
 - **ç†æƒ³æ–¹æ¡ˆ**ï¼šå¦‚æœå¯èƒ½ï¼Œç›´æ¥ä»æ•°æ®æºç«¯è¿›è¡Œé¢„å¤„ç†ï¼Œå°†çƒ­ç‚¹æ•°æ®æ‰“æ•£ã€‚ä¾‹å¦‚åœ¨ Hive ETL é˜¶æ®µå°±å¯¹é¢‘ç¹ä½¿ç”¨çš„ Key è¿›è¡ŒåŠ ç›æˆ–æ‰“æ•£ã€‚
 **b) è¿‡æ»¤å€¾æ–œçš„Key**  
 **æ–¹æ¡ˆ**ï¼šå¦‚æœæŸäº›çƒ­ç‚¹ Key ä¸æ˜¯ä¸šåŠ¡åˆ†ææ‰€å¿…éœ€çš„ï¼ˆä¾‹å¦‚çˆ¬è™«æŠ“å–çš„å¼‚å¸¸ NULL å€¼ã€æµ‹è¯•è´¦å·çš„æ•°æ®ï¼‰ï¼Œå¯ä»¥ç›´æ¥åœ¨ä½œä¸šä¸­è¿‡æ»¤æ‰å®ƒä»¬ã€‚
 **å‘½ä»¤ç¤ºä¾‹ï¼š**
 ```scala
 // å‡è®¾ 'key' åˆ—ä¸­å­˜åœ¨ä¸€äº›æˆ‘ä»¬ä¸éœ€è¦çš„å¼‚å¸¸å¤§Key
val filteredRDD = originalRDD.filter(row => row.getAs[String]("key") != "å¼‚å¸¸Keyå€¼")  
 ```
 
- c) ä¸¤é˜¶æ®µèšåˆï¼ˆåŠ ç›/æ‰“æ•£ -> èšåˆ -> å»ç› -> æœ€ç»ˆèšåˆï¼‰  
**åœºæ™¯ï¼š**é€‚ç”¨äº reduceByKey, groupByKey, agg ç­‰èšåˆç±» Shuffle æ“ä½œã€‚  
**æ­¥éª¤ï¼š**  
**- æ‰“æ•£ï¼š**ç»™æ¯ä¸ª Key åŠ ä¸Šä¸€ä¸ªéšæœºå‰ç¼€ï¼ˆç›ï¼‰ï¼Œå°†ä¸€ä¸ªå¤§ Key æ‹†åˆ†æˆå¤šä¸ªå° Keyã€‚  

```scala
// ç¬¬ä¸€æ­¥ï¼šåŠ ç›å±€éƒ¨èšåˆ
val saltedPairRDD = originalPairRDD.map{ case (key, value) =>
  val salt = (new util.Random).nextInt(numSalts) // numSalts æ˜¯éšæœºèŒƒå›´ï¼Œä¾‹å¦‚ 10
  (s"$salt-$key", value)
}
val partialAggRDD = saltedPairRDD.reduceByKey(_ + _) // å±€éƒ¨èšåˆ
```

**- å»ç›ï¼š**å»æ‰éšæœºå‰ç¼€ï¼Œæ¢å¤åŸå§‹ Keyã€‚
```scala
// ç¬¬äºŒæ­¥ï¼šå»ç›
val removedSaltRDD = partialAggRDD.map{ case (saltedKey, value) =>
  val originalKey = saltedKey.substring(saltedKey.indexOf("-") + 1)
  (originalKey, value)
}
```

**- æœ€ç»ˆèšåˆï¼š**å¯¹æ¢å¤åçš„åŸå§‹ Key è¿›è¡Œå…¨å±€èšåˆã€‚
```scala
// ç¬¬ä¸‰æ­¥ï¼šå…¨å±€èšåˆ
val finalAggRDD = removedSaltRDD.reduceByKey(_ + _)
```
**æ•ˆæœï¼š**å°†åŸæœ¬ç”±ä¸€ä¸ª Task å¤„ç†çš„ä¸€ä¸ªå¤§ Key çš„è®¡ç®—å‹åŠ›ï¼Œåˆ†æ‘Šç»™äº†å¤šä¸ª Taskï¼Œå®Œç¾è§£å†³å€¾æ–œã€‚

 - d) ä½¿ç”¨éšæœºKeyå®ç°æ‰©å®¹join  
  - **åœºæ™¯ï¼š**é€‚ç”¨äºå¤§è¡¨ Join å€¾æ–œç»´è¡¨ï¼ˆç»´åº¦è¡¨ä¸­æœ‰çƒ­ç‚¹ Keyï¼‰ï¼Œå³ Skew Joinã€‚
  - **æ­¥éª¤ï¼š**
    - ä»å¤§è¡¨ä¸­ç­›é€‰å‡ºå¯¼è‡´å€¾æ–œçš„çƒ­ç‚¹ Key åˆ—è¡¨ã€‚
    - æ‰“æ•£å¤§è¡¨ï¼šå°†å¤§è¡¨ä¸­çƒ­ç‚¹ Key çš„æ•°æ®åŠ ä¸Šéšæœºå‰ç¼€ï¼Œä»è€Œæ‰©å®¹ã€‚
    - æ‰©å®¹ç»´è¡¨ï¼šå°†ç»´è¡¨ä¸­çƒ­ç‚¹ Key çš„æ•°æ®å¤åˆ¶å¤šä»½ï¼ˆç¬›å¡å°”ç§¯ï¼‰ï¼Œæ¯ä»½å¯¹åº”ä¸€ä¸ªéšæœºå‰ç¼€ã€‚
    - Joinï¼šå°†å¤„ç†åçš„ä¸¤ä¸ªè¡¨è¿›è¡Œ Joinï¼Œç”±äºçƒ­ç‚¹ Key è¢«æ‰©å®¹åå¯ä»¥åŒ¹é…ä¸Šï¼Œéçƒ­ç‚¹ Key æ­£å¸¸ Joinã€‚
- ä»£ç æ€è·¯å¤æ‚ï¼ŒSpark 3.2+ å·²åŸç”Ÿæ”¯æŒ SKEW JOIN ä¼˜åŒ–ï¼Œå¯é€šè¿‡ Hint å®ç°ï¼š
  ```scala
  spark.sql("""
  SELECT /*+ SKEW('table_name', 'skewed_column') */ *
  FROM table_name
  """)

  ```
***åœ¨ä½ç‰ˆæœ¬ä¸­ï¼Œé€šå¸¸éœ€è¦æ‰‹åŠ¨å®ç°ä¸Šè¿°é€»è¾‘ã€‚***

##### ä¼˜åŒ–æ–¹å‘äºŒï¼šè°ƒæ•´åˆ†åŒºä¸å¹¶è¡Œåº¦
- a) æé«˜Shuffleå¹¶è¡Œåº¦
  **æ–¹æ¡ˆï¼š**é€šè¿‡è®¾ç½® spark.sql.shuffle.partitionsï¼ˆé»˜è®¤200ï¼‰æ¥å¢åŠ  Shuffle åçš„åˆ†åŒºæ•°ã€‚  
  **åŸç†ï¼š**è®©æ•°æ®è¢«åˆ†é…åˆ°æ›´å¤šä¸ª Task ä¸­å»å¤„ç†ï¼Œå³ä½¿æœ‰æ•°æ®å€¾æ–œï¼Œæ›´å¤§çš„åˆ†åŒºæ•°ä¹Ÿå¯èƒ½è®©å€¾æ–œç¨‹åº¦ç›¸å¯¹é™ä½ã€‚è¿™æ˜¯ä¸€ä¸ªç®€å•ä½†å¯èƒ½æœ‰æ•ˆçš„â€œç¼“å…µä¹‹è®¡â€ã€‚   
```scala
  spark.conf.set("spark.sql.shuffle.partitions", "1000") // æ ¹æ®æ•°æ®é‡è°ƒæ•´
   // æˆ–è€…åœ¨ reduceByKey ç­‰æ“ä½œä¸­ç›´æ¥æŒ‡å®šåˆ†åŒºæ•°
  rdd.reduceByKey(_ + _, 1000)
```
- b) ä½¿ç”¨è‡ªå®šä¹‰Partitioner
**æ–¹æ¡ˆï¼š**å¦‚æœä¸šåŠ¡é€»è¾‘æ¸…æ™°ï¼Œå¯ä»¥è‡ªå®šä¹‰åˆ†åŒºè§„åˆ™ï¼Œé¿å…æŸäº›åˆ†åŒºè½å…¥è¿‡å¤šæ•°æ®ã€‚
**åœºæ™¯ï¼š**ä¾‹å¦‚ï¼Œä½ æ˜ç¡®çŸ¥é“æŸäº› Key æ˜¯çƒ­ç‚¹ï¼Œå¯ä»¥ç¼–å†™è‡ªå·±çš„ Partitioner ç±»ï¼Œå°†è¿™äº› Key å¼ºåˆ¶åˆ†é…åˆ°å¤šä¸ªç‰¹å®šçš„åˆ†åŒºä¸­å»ã€‚


##### ä¼˜åŒ–æ–¹å‘ä¸‰ï¼šæ£€æŸ¥è®¡ç®—é€»è¾‘ä¸èµ„æº
- å¦‚æœä¸æ˜¯æ•°æ®é—®é¢˜ï¼Œè€Œæ˜¯è®¡ç®—é—®é¢˜ï¼š
  - **æ£€æŸ¥UDFï¼ˆç”¨æˆ·è‡ªå®šä¹‰å‡½æ•°ï¼‰**:ä½ çš„ UDF ä¸­æ˜¯å¦å­˜åœ¨ä½æ•ˆæ“ä½œï¼ˆå¦‚é¢‘ç¹åˆ›å»ºå¯¹è±¡ã€é€’å½’è¿‡æ·±ï¼‰ï¼Ÿæ˜¯å¦åœ¨æŸäº›ç‰¹å®šæ•°æ®ä¸Šä¼šè§¦å‘ä½æ•ˆè·¯å¾„ï¼Ÿä¼˜åŒ–ä½ çš„ä»£ç é€»è¾‘ã€‚
  - **æ£€æŸ¥èµ„æºç«äº‰**:
    - **GCï¼ˆåƒåœ¾å›æ”¶ï¼‰**:é•¿å°¾ Task å¯èƒ½å› ä¸ºå¤„ç†çš„æ•°æ®é‡å¤§ï¼Œè§¦å‘äº†é¢‘ç¹çš„ Full GCã€‚åœ¨ Spark UI ä¸­æ£€æŸ¥è¯¥ Task çš„ GC æ—¶é—´ã€‚è€ƒè™‘ä½¿ç”¨ G1GC å¹¶è°ƒæ•´å †å†…å­˜å’Œ GC å‚æ•°ã€‚
    - **Executor è´Ÿè½½ä¸å‡**:å¯èƒ½æŸä¸ª Executor æ‰€åœ¨çš„ç‰©ç†æœºè´Ÿè½½æœ¬èº«å°±å¾ˆé«˜ï¼ˆCPUã€ç£ç›˜IOã€ç½‘ç»œIOè¢«å…¶ä»–è¿›ç¨‹å ç”¨ï¼‰ï¼Œå¯¼è‡´ä¸Šé¢çš„æ‰€æœ‰ Task éƒ½å˜æ…¢ã€‚éœ€è¦ä»é›†ç¾¤ç›‘æ§å±‚é¢æ’æŸ¥ã€‚

##### ä¼˜åŒ–æ€»ç»“ä¸æµç¨‹
- å®šä½ï¼šä½¿ç”¨ Spark UI ç¡®å®šæ˜¯æ•°æ®å€¾æ–œè¿˜æ˜¯è®¡ç®—å€¾æ–œã€‚
- é¦–é€‰ï¼šå¦‚æœèƒ½è¿‡æ»¤æ‰å€¾æ–œKeyï¼Œè¿™æ˜¯æœ€ç›´æ¥çš„æ–¹æ³•ã€‚
- æ ¸å¿ƒæ‰‹æ®µï¼šå¯¹äºèšåˆæ“ä½œï¼Œä¼˜å…ˆè€ƒè™‘ä¸¤é˜¶æ®µèšåˆï¼ˆåŠ ç›ï¼‰ï¼›å¯¹äº Join æ“ä½œï¼Œä¼˜å…ˆçœ‹èƒ½å¦ä½¿ç”¨ Spark 3.2+ çš„ SKEW JOIN Hintã€‚
- é€šç”¨æŠ€å·§ï¼šå°è¯•å¢åŠ  Shuffle åˆ†åŒºæ•° (spark.sql.shuffle.partitions)ã€‚
- æ·±åº¦ä¼˜åŒ–ï¼šè€ƒè™‘è‡ªå®šä¹‰åˆ†åŒºå™¨æˆ–ä¼˜åŒ–UDF ä»£ç å’Œ JVM å‚æ•°ã€‚
- é•¿å°¾é—®é¢˜çš„ä¼˜åŒ–é€šå¸¸æ˜¯ä¸Šè¿°å¤šç§æ–¹æ³•ç»“åˆä½¿ç”¨ã€åå¤è¿­ä»£çš„è¿‡ç¨‹ã€‚æ ¸å¿ƒæ€æƒ³æ°¸è¿œæ˜¯ï¼šå°†é›†ä¸­åœ¨ä¸€å¤„çš„è®¡ç®—å’Œå­˜å‚¨å‹åŠ›ï¼Œå°½å¯èƒ½åœ°åˆ†æ•£åˆ°å¤šä¸ªå¹¶è¡Œå•å…ƒä¸­å»ã€‚

#### taské•¿å°¾æ¡ˆä¾‹
è¿™é‡Œæœ‰ä¸ªè¿è¡Œæ—¶é•¿ä¸º20.03mï¼Œä¸­ä½åªæœ‰0.90s  
![alt text](img/task_01.png)
sparkè¿™é‡Œä¹Ÿèƒ½æ˜ç¡®çœ‹å‡ºæ¥
![alt text](img/task_02.png)
```
...  
WHERE   dt = '20250910' and hour >= '13' and hour < '14' 
...  
-- sqlè¯­æ³•çš„æ¡ä»¶ä¸º houréåˆ†åŒº,åˆ†é’Ÿåˆ†åŒºä¸ºhm,è¿™é‡Œä¼šæ‰«æ20250910å…¨åˆ†åŒºçš„æ•°æ®ï¼Œæ‰€ä»¥åªæœ‰å‡ ç§’çš„taskå…¶å®æ˜¯è¯»äº†é13 14åˆ†åŒºçš„æ–‡ä»¶ï¼Œä½†æ²¡æœ‰ä»»ä½•æ•°æ®input,å±äºæµªè´¹èµ„æºç©ºè·‘ã€‚  

containeræ—¥å¿—ï¼š  
25/09/10 15:01:40 INFO FileScanRDD: Reading File path: hdfs://xxcluster01/hive_warehouse/xx.db/tab/dt=20250910/hm=0529/bc_27_merge_1757453497469_0.zlib, range: 268435456-378628974, partition values: [20250910,0529]  
```
**è§£å†³æ–¹æ¡ˆï¼š**  
1: å’Œä¸šåŠ¡ç¡®è®¤äº†hmå’Œhouræ˜¯å¯¹ç­‰å…³ç³»ï¼Œæ‰€ä»¥åªéœ€è¦æ¢æˆhmå³å¯ã€‚å¦‚æœéå¯¹ç­‰å…³ç³»ï¼Œéœ€è¦å…¶å®ƒä¼˜åŒ–æ‰‹æ®µ    
2: è¿™é‡Œå¯¹åŸæ•°æ®é‡æ–°åˆ†åŒºï¼Œé¿å…åº•å±‚æ•°æ®å€¾æ–œ,è¿™é‡Œæ²¡æœ‰æŒ‡å®šåˆ†åŒºæ•°ï¼Œé¿å…æœ€å°æ”¹åŠ¨åŸåˆ™ï¼Œä¼šç”¨å…¨å±€ç»Ÿè®¡é…ç½®æ˜¯500(spark.sql.shuffle.partitions=500)ï¼Œä¸šåŠ¡å¯ä»¥æ ¹æ®è‡ªå·²çš„æ•°æ®é‡åˆç†çš„è®¾ç½®ï¼Œä¾‹ï¼š/*+ REPARTITION(200) */  
```
SELECT   /*+ REPARTITION */
 col1,col2.... from tab
```
3ï¼šæ˜¾å¼ç¼“å­˜è¡¨ æœ‰å¤šæ¬¡é‡å¤æŸ¥è¯¢,ä¾‹ï¼š
```
CACHE TABLE details_data OPTIONS ('storageLevel' = 'DISK_ONLY') AS SELECT * FROM details_data_view;
```  
**ä¼˜åŒ–æ•ˆæœ**
- åŸä»»åŠ¡45åˆ†å·¦å³ï¼Œä¼˜åŒ–å6åˆ†å·¦å³è·‘å®Œ
- å†…å­˜æ—¶ä¼˜åŒ–å‰1053474.5	ä¼˜åŒ–åï¼š249052.08 
- ä¼˜åŒ–åçš„taskå¯¹æ¯” åŸtaskä¸¥é‡å€¾æ–œï¼Œä¼˜åŒ–åå¯ä»¥å¹³å‡å¤„ç†input shuufleæ•°æ®     
![alt text](img/image-4.png) 

#### taské•¿å°¾æ¡ˆä¾‹2
- compass ä¸Šçœ‹å‡ºæ—¶é—´ç›¸å·®å¾ˆå¤šï¼Œæ˜¾ç¤ºtaské•¿å°¾é—®é¢˜
![alt text](image-7.png)
- é€šè¿‡spark sqlæ‰§è¡Œè®¡åŒ–æ‰¾å‡ºæ—¶é—´æœ€å¤§çš„stage,  
![alt text](7D8A8DE6BAABC6139FDF9AD7700287B2.jpg)
é€šè¿‡æ‰§è¡Œè®¡åŒ–å·²ç»å¾ˆæ˜æ˜¾çœ‹å‡ºæ˜¯AQEåˆ†åŒºcoalescedäº†ï¼šnumber of coalesced partitions: 4  
å†å…³è”æ€§æ‰¾åˆ°sqlæ‰§è¡Œç‰‡æ®µ  
![alt text](image-8.png)
- ä¼˜åŒ–é—®é¢˜
  - 1ï¼šæ‰‹åŠ¨åˆ†åŒºï¼Œé¿å…AQE coalesced
  - 2ï¼šcoalesce ä¸ä¼šè°“è¯ä¸‹æ¨ï¼ˆPredicate Pushdownï¼‰ï¼Œå¯¼è‡´ä¼šå¤šè¯»å‡ºå¾ˆå¤šæ•°æ®
![alt text](8BDBE81DACFB6C1DB9202C173396B1BD.jpg)   

```sql
-- hdid is not null and hdid != '' å†™æ³•ç”Ÿæˆçš„æ‰§è¡Œè®¡åŒ–
PushedFilters: [In(mtr_src_type, [ä¸‰æ–¹,å®˜æ–¹]), IsNotNull(hdid), Not(EqualTo(hdid,))]

-- coalesce(hdid, '') <> ''  å†™æ³•çš„æ‰§è¡Œè®¡åŒ–
PushedFilters: [In(mtr_src_type, [ä¸‰æ–¹,å®˜æ–¹])]

-- è¿™é‡Œå·²ç»å¾ˆæ˜ç¡®çš„çœ‹å‡ºorcæ–‡ä»¶ä¸çŸ¥é“coalesceæ˜¯å•¥ç©æ„ï¼Œå°±ä¼šæŠŠhdidå…¨éƒ¨æ•°æ®æå‡ºæ¥ï¼Œä½ è‡ªå·²å»coalesceè¿‡æ»¤ï¼Œæ€§èƒ½å·®
```

- ä¼˜åŒ–å‰åå¯¹æ¯”ï¼Œä¼˜åŒ–å‰1å°æ—¶å·¦å³ï¼Œä¼˜åŒ–å21åˆ†é’Ÿè·‘å®Œ   
![alt text](image-10.png)
![alt text](image-9.png)

#### taské•¿å°¾æ¡ˆä¾‹3
1ï¼š è¿™é‡Œæ˜¾ç¤ºjob[17].stage[18].task[2423]è¿è¡Œè€—æ—¶48.22s ä¸­ä½å€¼ä¸º0.08s  
![alt text](image-12.png)
- æŸ¥çœ‹å¯¹åº”çš„sqlæ‰§è¡Œè®¡åŒ–ï¼Œè¿™é‡Œå‘ç°readçš„æ—¶å€™æºè¡¨åªä¸€ä¸ª2kå·¦å³çš„æ–‡ä»¶ï¼Œç¡®å®æ˜¯ä¸€ä¸ªåˆ†åŒº
![alt text](image-13.png)
- æ‰¾åˆ°å¯¹åº”çš„sqlæ‰§è¡Œæ®µï¼Œè¿™é‡Œunionåé¢çš„è¡¨çš„æ—¶å€™,æœ‰ä¸ªgroup by,è¿™æ—¶å€™ä¼šè§¦å‘shuufleé‡åˆ†åŒºè¿›è¡Œå»é‡ï¼Œä¼šæŒ‰å½“å‰é»˜è®¤500åˆ†åŒºè¿›è¡Œåˆ‡ï¼Œä½†å¯¹äºkbå‡ å€çš„åˆ‡500åˆ†åŒºï¼Œæ˜¯æµªè´¹çš„ï¼Œéœ€è¦æ‰‹åŠ¨åˆ†åŒº 
```select  /*+ REPARTITION(1) */ ...```
![alt text](image-14.png)

2ï¼š job[26].stage[34].task[4911]è¿è¡Œè€—æ—¶1.98m ä¸­ä½å€¼ä¸º1.25s  
- æ‰¾åˆ°æ‰§è¡Œè®¡åˆ’å’Œå¯¹åº”çš„sqlï¼Œè¿™ç§å†™æ³•æ²¡èƒ½è°“è¯ä¸‹æ¨ï¼Œä»æ‰§è¡Œè®¡åŒ–å›¾ä¸­å¯ä»¥æ˜æ˜¾ç¤ºçœ‹å‡º,æœ‰68,360,440æ¡æ•°æ®è¢«ColumnarToRow è¿›è¡Œfliter
![alt text](image-15.png)
![alt text](image-16.png)
```sql
-- è¿™é‡Œæ˜æ˜¾æ˜¯æ²¡æœ‰è¿›è¡Œ PushedFilters 
(104) Scan orc pub_dw.pub_dwv_live_view_btype_view_dr_di
Output [8]: [live_prod_name#9455, aid#9459L, uid#9466L, suid#9467, view_prod_name#9468, view_dr#9476, bste_act_type#9479, dt#9490]
Batched: true
Location: InMemoryFileIndex [hdfs://yycluster02/hive_warehouse/pub_dw.db/pub_dwv_live_view_btype_view_dr_di/dt=2025-09-18]
PartitionFilters: [isnotnull(dt#9490), (dt#9490 = 2025-09-18)]
PushedFilters: [IsNotNull(bste_act_type), EqualTo(bste_act_type,0)]
ReadSchema: struct<live_prod_name:string,aid:bigint,uid:bigint,suid:string,view_prod_name:string,view_dr:int,bste_act_type:int>

 -- ä¼˜åŒ–sqlå†™æ³•

-- and a.live_prod_name in ('YY',if(a.dt < '2022-04-06','bdgame','bdsdk'),'sdk_voiceroom')
  AND (
    -- æ›¿ä»£ live_prod_name IN çš„åŠ¨æ€é€»è¾‘
    a.live_prod_name = 'YY'
    OR a.live_prod_name = 'sdk_voiceroom'
    OR (
      a.dt < '2022-04-06' AND a.live_prod_name = 'bdgame'
    )
    OR (
      a.dt >= '2022-04-06' AND a.live_prod_name = 'bdsdk'
    )
  )

-- ä¼˜åŒ–åçš„æ‰§è¡Œè®¡åŒ–ï¼Œlive_prod_nameæˆåŠŸè°“è¯ä¸‹æ¨,
(107) Scan orc pub_dw.pub_dwv_live_view_btype_view_dr_di
Output [8]: [live_prod_name#9467, aid#9471L, uid#9478L, suid#9479, view_prod_name#9480, view_dr#9488, bste_act_type#9491, dt#9502]
Batched: true
Location: InMemoryFileIndex [hdfs://yycluster02/hive_warehouse/pub_dw.db/pub_dwv_live_view_btype_view_dr_di/dt=2025-09-18]
PartitionFilters: [isnotnull(dt#9502), (dt#9502 = 2025-09-18)]
PushedFilters: [IsNotNull(bste_act_type), Or(Or(EqualTo(live_prod_name,YY),EqualTo(live_prod_name,sdk_voiceroom)),Or(EqualTo(live_prod_name,bdgame),EqualTo(live_prod_name,bdsdk))), EqualTo(bste_act_type,0)]

ReadSchema: struct<live_prod_name:string,aid:bigint,uid:bigint,suid:string,view_prod_name:string,view_dr:int,bste_act_type:int>

```
and (include('SDK_PROD',a.view_prod_name) = 1or a.view_prod_name = 'bdbaizhan')  
è¿™ç§å› ä¸ºä½¿ç”¨äº†UDFå‡½æ•°ï¼Œä¹Ÿä¸èƒ½è°“è¯ä¸‹æ¨ï¼Œä»æ‰§è¡Œè®¡åŒ–é‡Œå¯ä»¥çœ‹åˆ°æœ‰(output rows-filter=68,360,440)æ¡æ•°æ®è¢«ColumnarToRowï¼ŒUDFé€»è¾‘åªæœ‰ä¸šåŠ¡æ¸…æ¥šï¼Œå¦‚æœéœ€è¦æ›´ä¼˜åŒ–çš„æ€§èƒ½ï¼Œéœ€è¦ä¸šåŠ¡è¿›è¡Œå…ˆ view_prod_name in(xx,xx)å†è¿›è¡Œudfè½¬æ¢è¿‡æ»¤ã€‚  
**UDFä½¿ç”¨æç¤ºï¼š**
é»‘ç›’è®¾è®¡ï¼šUDFçš„è®¾è®¡åˆè¡·æ˜¯ä¸ºäº†å…è®¸ç”¨æˆ·æ‰©å±•Sparkçš„åŠŸèƒ½ï¼Œä½†ä»Sparkçš„è§’åº¦æ¥çœ‹ï¼ŒUDFæ˜¯ä¸€ä¸ªé»‘ç›’ã€‚Sparkå¼•æ“æ— æ³•ç†è§£UDFå†…éƒ¨çš„è®¡ç®—é€»è¾‘ï¼Œä¹Ÿå°±æ— æ³•è¿›è¡Œæ·±å…¥çš„ä¼˜åŒ–ã€‚  
æ— æ³•åˆ©ç”¨Sparkçš„å†…éƒ¨ä¼˜åŒ–ï¼š:UDFåœ¨Sparkçœ‹æ¥æ˜¯ä¸€ä¸ªé»‘ç›’ï¼Œå®ƒæ— æ³•ç†è§£UDFå†…éƒ¨çš„è®¡ç®—é€»è¾‘ï¼Œå› æ­¤æ— æ³•å¯¹UDFè¿›è¡Œå¦‚ä»£ç ä¼˜åŒ–ã€å‘é‡åŒ–ç­‰æ“ä½œï¼Œä»è€Œé”™å¤±äº†åˆ©ç”¨Sparkå¼•æ“çš„ä¼˜åŒ–èƒ½åŠ›ã€‚  
**æ›¿ä»£æ–¹æ¡ˆï¼š**
ä¸ºäº†å…‹æœè¿™äº›ç¼ºé™·ï¼Œå¯ä»¥è€ƒè™‘ä½¿ç”¨ä»¥ä¸‹æ–¹æ¡ˆï¼š  
* ä½¿ç”¨Spark å†…ç½®å‡½æ•°ï¼šä¼˜å…ˆä½¿ç”¨Sparkæä¾›çš„å†…ç½®å‡½æ•°ï¼Œå®ƒä»¬é€šå¸¸å·²ç»ç»è¿‡é«˜åº¦ä¼˜åŒ–ï¼Œæ€§èƒ½è¿œé«˜äºUDFã€‚  
* ä½¿ç”¨Scala/Java UDFï¼šå¦‚æœå¿…é¡»è‡ªå®šä¹‰å‡½æ•°ï¼Œå¯ä»¥ä¼˜å…ˆä½¿ç”¨Scalaæˆ–Javaç¼–å†™çš„UDFï¼Œå› ä¸ºå®ƒä»¬ä¸éœ€è¦è¿›è¡Œè·¨è¯­è¨€çš„åºåˆ—åŒ–/ååºåˆ—åŒ–ï¼Œæ€§èƒ½ä¼šå¥½å¾ˆå¤šã€‚  
* ä½¿ç”¨Pandas UDF(Vectorized UDF)ï¼šå¯¹äºPython UDFï¼Œå¯ä»¥ä½¿ç”¨Pandas UDFï¼Œå®ƒä»¥Apache Arrowä¸ºåŸºç¡€ï¼Œå°†æ•°æ®æ‰“åŒ…æˆPandas Series/DataFrameï¼Œç„¶åä¸€æ¬¡æ€§è¿›è¡Œå¤„ç†ï¼Œé¿å…äº†è¡Œçº§åˆ«çš„åºåˆ—åŒ–å’Œååºåˆ—åŒ–å¼€é”€ï¼Œæ€§èƒ½æœ‰æ‰€æå‡ã€‚  


- ä¼˜åŒ–åæ—¶é—´å¯¹æ¯” 43 vs 11
![alt text](image-17.png)

#### taské•¿å°¾æ¡ˆä¾‹4
- task 2212 7åˆ†é’Ÿï¼Œoutputå’Œshuufleæ•°æ®é‡è¿˜å°
- task 2211 5åˆ†é’Ÿï¼Œ outputå’Œshuufleæ•°æ®é‡å¤§ï¼Œä½†æ—¶é—´è¿˜å¿«ã€‚è¿™ä¸æ­£å¸¸ 
![alt text](image-11.png)  
spark å†™å…¥çš„2ä¸ªtaskæ–‡ä»¶ 

```text
46.3 M  138.8 M  hdfs://xx/hive_warehouse/xx.db/livevip_dws_entity_eqmt_mtr_mall_stat_180d_di/dt=2025-09-18/part-00000-c407b8f4-c9be-462f-90dd-2e32ba9a6df1-c000
46.3 M  138.9 M  hdfs://xx/hive_warehouse/xx.db/livevip_dws_entity_eqmt_mtr_mall_stat_180d_di/dt=2025-09-18/part-00001-c407b8f4-c9be-462f-90dd-2e32ba9a6df1-c000

10.12.68.133 task å¿« spark task 2 å†™å…¥
57:10-52:06=5:04
25/09/19 03:52:06 INFO WriterImpl: ORC writer created for path: hdfs://xx/hive_warehouse/xx.db/livevip_dws_entity_eqmt_mtr_mall_stat_180d_di/.hive-staging_hive_2025-09-19_03-28-47_267_4485193128334077227-1/-ext-10000/_temporary/0/_temporary/attempt_202509190351587536033476281976156_0068_m_000002_2214/part-00002-c407b8f4-c9be-462f-90dd-2e32ba9a6df1-c000 with stripeSize: 67108864 options: Compress: SNAPPY buffer: 65536
25/09/19 03:57:10 INFO FileOutputCommitter: Saved output of task 'attempt_202509190351587536033476281976156_0068_m_000002_2214' to hdfs://xx/hive_warehouse/xx.db/livevip_dws_entity_eqmt_mtr_mall_stat_180d_di/.hive-staging_hive_2025-09-19_03-28-47_267_4485193128334077227-1/-ext-10000
25/09/19 03:57:10 INFO SparkHadoopMapRedUtil: attempt_202509190351587536033476281976156_0068_m_000002_2214: Committed
25/09/19 03:57:10 INFO Executor: Finished task 2.0 in stage 68.0 (TID 2214). 19524 bytes result sent to driver

10.12.68.18 æ…¢spark task 1 å†™å…¥æ—¥å¿—
58:59-52:15=6:34
25/09/19 03:52:15 INFO WriterImpl: ORC writer created for path: hdfs://xx/hive_warehouse/x.db/livevip_dws_entity_eqmt_mtr_mall_stat_180d_di/.hive-staging_hive_2025-09-19_03-28-47_267_4485193128334077227-1/-ext-10000/_temporary/0/_temporary/attempt_202509190351584893227535942616307_0068_m_000001_2212/part-00001-c407b8f4-c9be-462f-90dd-2e32ba9a6df1-c000 with stripeSize: 67108864 options: Compress: SNAPPY buffer: 65536
25/09/19 03:58:59 INFO FileOutputCommitter: Saved output of task 'attempt_202509190351584893227535942616307_0068_m_000001_2212' to hdfs://xx/hive_warehouse/xx.db/livevip_dws_entity_eqmt_mtr_mall_stat_180d_di/.hive-staging_hive_2025-09-19_03-28-47_267_4485193128334077227-1/-ext-10000
25/09/19 03:58:59 INFO SparkHadoopMapRedUtil: attempt_202509190351584893227535942616307_0068_m_000001_2212: Committed
25/09/19 03:58:59 INFO Executor: Finished task 1.0 in stage 68.0 (TID 2212). 19524 bytes result sent to driver

æ…¢çš„æ–‡ä»¶
hdfs fsck hdfs://xx/hive_warehouse/xx.db/livevip_dws_entity_eqmt_mtr_mall_stat_180d_di/dt=2025-09-18/part-00001-c407b8f4-c9be-462f-90dd-2e32ba9a6df1-c000 -files -blocks -locations
Connecting to namenode via http://fs-hiido-xx-yynn2.hiido.host.int.yy.com:53070/fsck?ugi=hdfs&files=1&blocks=1&locations=1&path=%2Fhive_warehouse%2Fxx.db%2Flivevip_dws_entity_eqmt_mtr_mall_stat_180d_di%2Fdt%3D2025-09-18%2Fpart-00001-c407b8f4-c9be-462f-90dd-2e32ba9a6df1-c000
FSCK started by hdfs (auth:KERBEROS_SSL) from /10.12.68.182 for path /hive_warehouse/xx.db/livevip_dws_entity_eqmt_mtr_mall_stat_180d_di/dt=2025-09-18/part-00001-c407b8f4-c9be-462f-90dd-2e32ba9a6df1-c000 at Fri Sep 19 10:52:55 CST 2025
/hive_warehouse/xx.db/livevip_dws_entity_eqmt_mtr_mall_stat_180d_di/dt=2025-09-18/part-00001-c407b8f4-c9be-462f-90dd-2e32ba9a6df1-c000 48551864 bytes, replicated: replication=3, 1 block(s):  OK
0. BP-1159253446-10.21.118.29-1568116770575:blk_5688236938_4994280941 len=48551864 Live_repl=3  [DatanodeInfoWithStorage[10.12.64.3:1019,DS-13c2dcdc-a665-4729-9e21-cffa544b5906,DISK], DatanodeInfoWithStorage[10.12.64.74:1019,DS-a4d7b2a7-ddda-4ddc-9ae0-ca9639836a63,DISK], DatanodeInfoWithStorage[10.12.68.18:1019,DS-2a82ab06-9482-4363-9360-d67681206333,DISK]]

ip:10.12.68.18
2025-09-19 03:52:39,146 INFO  datanode.DataNode (DataXceiver.java:writeBlock(738)) - Receiving BP-1159253446-10.21.118.29-1568116770575:blk_5688236938_4994280941 src: /10.12.68.18:37166 dest: /10.12.68.18:1019
2025-09-19 03:58:59,645 INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /10.12.68.18:37166, dest: /10.12.68.18:1019, bytes: 48551864, op: HDFS_WRITE, cliID: DFSClient_attempt_202509190351584893227535942616307_0068_m_000001_2212_1350099200_47, offset: 0, srvID: 7141b89d-984a-4f49-a211-2a29c33c603c, blockid: BP-1159253446-10.21.118.29-1568116770575:blk_5688236938_4994280941, duration(ns): 380461993891
2025-09-19 03:58:59,645 INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1159253446-10.21.118.29-1568116770575:blk_5688236938_4994280941, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[10.12.64.74:1019, 10.12.64.3:1019] terminating

ip:10.12.64.74
2025-09-19 03:52:39,156 INFO  datanode.DataNode (DataXceiver.java:writeBlock(738)) - Receiving BP-1159253446-10.21.118.29-1568116770575:blk_5688236938_4994280941 src: /10.12.68.18:39452 dest: /10.12.64.74:1019
2025-09-19 03:58:59,628 INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /10.12.68.18:39452, dest: /10.12.64.74:1019, bytes: 48551864, op: HDFS_WRITE, cliID: DFSClient_attempt_202509190351584893227535942616307_0068_m_000001_2212_1350099200_47, offset: 0, srvID: 71b081c8-3cd9-4bf8-be76-12961de006da, blockid: BP-1159253446-10.21.118.29-1568116770575:blk_5688236938_4994280941, duration(ns): 380462834937
2025-09-19 03:58:59,628 INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1159253446-10.21.118.29-1568116770575:blk_5688236938_4994280941, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.12.64.3:1019] terminating

ip:10.12.64.3
2025-09-19 03:52:39,171 INFO  datanode.DataNode (DataXceiver.java:writeBlock(738)) - Receiving BP-1159253446-10.21.118.29-1568116770575:blk_5688236938_4994280941 src: /10.12.64.74:28970 dest: /10.12.64.3:1019
2025-09-19 03:58:59,642 INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /10.12.64.74:28970, dest: /10.12.64.3:1019, bytes: 48551864, op: HDFS_WRITE, cliID: DFSClient_attempt_202509190351584893227535942616307_0068_m_000001_2212_1350099200_47, offset: 0, srvID: 9f591b64-46d5-40fe-8b65-177a13a2aba8, blockid: BP-1159253446-10.21.118.29-1568116770575:blk_5688236938_4994280941, duration(ns): 380465797665
2025-09-19 03:58:59,642 INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-1159253446-10.21.118.29-1568116770575:blk_5688236938_4994280941, type=LAST_IN_PIPELINE terminating

```
part-00001-c407b8f4-c9be-462f-90dd-2e32ba9a6df1-c000 ï¼Œ(2025-09-19 03:52:39.xx)æ¥æ”¶,åŒç§’é’Ÿ(2025-09-19 03:58:59.xx)finalize 
ä»æ—¶é—´ä¸Šçœ‹ï¼Œç®¡é“å»ºç«‹é˜¶æ®µæ˜¯å·®ä¸å¤šçš„ï¼Œæ•°æ®å®Œæˆæ—¶é—´ä¹Ÿæ˜¯å·®ä¸å¤šçš„ï¼Œé‚£å°±å¯ä»¥è¯æ˜é—®é¢˜å‡ºåœ¨å®¢æˆ·ç«¯ä¸Šï¼Œå®¢æˆ·ç«¯å°±æ˜¯ç¬¬ä¸€ä¸ªå†™å…¥datanodeçš„èŠ‚ç‚¹10.12.68.18ï¼ŒæŸ¥çœ‹è¿™å°æœåŠ¡å™¨çš„ç›‘æ§æŒ‡æ ‡ï¼Œå‘ç°åˆšå¥½è¿™ä¸ªæ—¶é—´ç‚¹ä¸Šçš„æŸå‡ å—ç£ç›˜io 100%ã€‚è¯´ä»¥å¯¼è‡´å†™æ–‡ä»¶å˜æ…¢ã€‚
å› ä¸ºä¸€å°æœåŠ¡å™¨ä¸ŠåŒæ—¶éƒ¨ç½²æœ‰dn\nm\shuufleæœåŠ¡ï¼Œä¸èƒ½ç¡®å®šæ˜¯é‚£ä¸ªæœåŠ¡å¯¼è‡´io 100%çš„é—®é¢˜ï¼Œè¿™ä¸ªåªèƒ½å®ç°ç”¨cgroupæ¥éš”ç¦»ç£ç›˜io,å¾…ä¼˜åŒ–


### æ•°æ®å€¾æ–œ

 **æè¿°:** æ•°æ®å€¾æ–œè¯Šæ–­è§„åˆ™å¦‚ä¸‹  
   1ã€ä»»åŠ¡æ€»è€—æ—¶>30min  
   2ã€stageè€—æ—¶/ä»»åŠ¡æ€»è€—æ—¶>45%  
   3ã€shuffle readçš„æ•°æ®é‡æ»¡è¶³ä¸€ä¸‹æ¡ä»¶ä¹‹ä¸€ï¼š  

  ```yaml
  # 0w-5w
  - start: 0
    end: 50000
    threshold: 0
  # 5w-10w
  - start: 50000
    end: 100000
    threshold: 100
  # 10w-100w
  - start: 100000
    end: 1000000
    threshold: 50
  # 100w-500w
  - start: 1000000
    end: 5000000
    threshold: 10
  # 500w-2000w
  - start: 5000000
    end: 20000000
    threshold: 5
  # 2000w-3000w
  - start: 20000000
    end: 30000000
    threshold: 3.5
  # 3000w-4000w
  - start: 30000000
    end: 40000000
    threshold: 3
  # 4000w-5000w
  - start: 40000000
    end: 50000000
    threshold: 2.25
  # 5000w
  - start: 50000000
    end: 0
    threshold: 2
  ```
#### è®¡ç®—å…¬å¼    

  - Sparkæ•°æ®å€¾æ–œçš„è®¡ç®—å…¬å¼ä¸»è¦æ˜¯é€šè¿‡æ¯”è¾ƒæ¯ä¸ªstageä¸­taskçš„shuffle readæ•°æ®é‡çš„æœ€å¤§å€¼(max)å’Œä¸­ä½æ•°(median)çš„æ¯”å€¼æ¥åˆ¤æ–­ã€‚  
    å…·ä½“å…¬å¼ä¸ºï¼šæ•°æ®å€¾æ–œæ¯”ä¾‹ = max(shuffle_read) / median(shuffle_read)
  - ä¸¾ä¾‹è¯´æ˜ï¼š
    - å‡è®¾æŸä¸ªstageæœ‰5ä¸ªtaskï¼Œå®ƒä»¬çš„shuffle readæ•°æ®é‡åˆ†åˆ«ä¸ºï¼š[1000, 1200, 1100, 1050, 5000]
    - ä¸­ä½æ•°medianä¸º1100ï¼Œæœ€å¤§å€¼maxä¸º5000
    - æ•°æ®å€¾æ–œæ¯”ä¾‹ = 5000/1100 â‰ˆ 4.55
    - å¦‚æœé…ç½®çš„é˜ˆå€¼æ˜¯3ï¼Œé‚£ä¹ˆè¿™ä¸ªstageå°±ä¼šè¢«æ ‡è®°ä¸ºå­˜åœ¨æ•°æ®å€¾æ–œ(abnormal)
  - è¯Šæ–­å¹³å°é˜ˆå€¼

#### å»ºè®®ä¼˜åŒ–
**é’ˆå¯¹Sparkæ•°æ®å€¾æ–œé—®é¢˜ï¼Œå¸¸è§çš„ä¼˜åŒ–æ–¹æ³•åŒ…æ‹¬ï¼š**  
  - å¢åŠ åˆ†åŒºæ•° ï¼šé€šè¿‡ repartition æˆ– coalesce å¢åŠ åˆ†åŒºæ•°é‡ï¼Œä½¿æ•°æ®åˆ†å¸ƒæ›´å‡åŒ€
  - ä½¿ç”¨éšæœºå‰ç¼€ ï¼šå¯¹å€¾æ–œçš„keyæ·»åŠ éšæœºå‰ç¼€ï¼Œåˆ†æ•£æ•°æ®åˆ°ä¸åŒåˆ†åŒº
  - å¹¿æ’­å°è¡¨ ï¼šå¯¹äºjoinæ“ä½œä¸­çš„å°è¡¨ï¼Œä½¿ç”¨å¹¿æ’­å˜é‡é¿å…shuffle
  - ä¸¤é˜¶æ®µèšåˆ ï¼šå…ˆå¯¹å€¾æ–œkeyè¿›è¡Œå±€éƒ¨èšåˆï¼Œå†è¿›è¡Œå…¨å±€èšåˆ
  - è¿‡æ»¤å€¾æ–œkey ï¼šå•ç‹¬å¤„ç†å€¾æ–œkeyï¼Œå†åˆå¹¶ç»“æœ
  - è°ƒæ•´å¹¶è¡Œåº¦ ï¼šé€šè¿‡ spark.sql.shuffle.partitions å‚æ•°è°ƒæ•´shuffleå¹¶è¡Œåº¦
  - å¦å‚è€ƒ *   [ä¼˜åŒ–æ–¹å‘ä¸€ï¼šåº”å¯¹æ•°æ®å€¾æ–œ (Data Skewness)](#ä¼˜åŒ–æ–¹å‘ä¸€ï¼šåº”å¯¹æ•°æ®å€¾æ–œ (Data Skewness))  
  ä¾‹ï¼šæŠŠä¸ªsaprk sql è¯†åˆ«å‡ºæ•°æ®å€¾æ–œï¼ŒæŠŠæ‰§è¡Œè®¡åŒ–ä¸¢ç»™AI  
  ![alt text](image-2.png)
  ![alt text](image-1.png)  

  ä¼˜åŒ–åçš„sql:  
  è¿™é‡Œæ˜¯æ ¹æ®session_idåˆ†ç»„æ±‚ pathæœ€é•¿çš„ä¸€æ¡ï¼Œæ‰€ä»¥nullå€¼ä¹Ÿåªä¼šå–ä¸€æ¡ï¼Œè¿™é‡Œå¯ä»¥ç›´æ¥å¹²æ‰ï¼Œæˆ–æŠŠnullå€¼å•ç‹¬å»æ±‚ä¸€ä¸ªmax  
  
  ```sql
  select
    xxx,
    row_number() over(partition by session_id  order by    length(path) desc ) as row_id
  from   raw_expand_data where session_id is not null
  ```

 **ä»£ç å®ç°ç¤ºä¾‹ï¼š**  
 ```java
 // å¢åŠ åˆ†åŒºæ•°
Dataset<Row> repartitioned = dataset.repartition(200);
// å¹¿æ’­å°è¡¨
Dataset<Row> smallTable = ...;
Dataset<Row> bigTable = ...;
Dataset<Row> joined = bigTable.join(broadcast(smallTable), "key");
// ä¸¤é˜¶æ®µèšåˆ
Dataset<Row> stage1 = dataset.groupBy("key").agg(sum("value"));
Dataset<Row> stage2 = stage1.groupBy("key").agg(sum("sum(value)"));
 ```

### å¤§è¡¨æ‰«æ
æ‰¾åˆ°å¯¹åº”çš„è¡¨sqlï¼Œçœ‹æ˜¯å¦æœ‰å¼‚å¸¸  
![alt text](CC0323778B0680E0C11089001CD53F7B.jpg)

### æ¨æµ‹æ‰§è¡Œè¿‡å¤šåˆ†æ
**Stageä¸­æ¨æµ‹æ‰§è¡Œä»»åŠ¡æ•°è¶…è¿‡20ä¸ªï¼Œå³å¯åˆ¤å®šä¸ºæ¨æµ‹æ‰§è¡Œè¿‡å¤š**
![alt text](image.png)

**å½“å‰spark3.2.1**
```sql
spark.speculation	true
spark.speculation.interval	5s
spark.speculation.multiplier	2
--3.2.1é»˜è®¤å€¼ 
spark.speculation.multiplier 1.5 //ä¸€é¡¹ä»»åŠ¡çš„é€Ÿåº¦è¦æ¯”å¹³å‡é€Ÿåº¦æ…¢å¤šå°‘å€ï¼Œæ‰èƒ½è¢«çº³å…¥æ¨æµ‹æ‰§è¡Œçš„èŒƒå›´ä¹‹å†…ã€‚
spark.speculation.quantile	0.75 //åœ¨ä¸ºç‰¹å®šé˜¶æ®µå¯ç”¨æ¨æµ‹ä¹‹å‰å¿…é¡»å®Œæˆçš„ä»»åŠ¡æ¯”ä¾‹ã€‚
```

**å»ºè®®ä¼˜åŒ–**
- æé«˜ä¹˜æ•°å› å­ (spark.speculation.multiplier=3)  
  ä½¿åˆ¤æ–­æ ‡å‡†æ›´ä¸¥æ ¼ï¼Œåªæœ‰æ˜æ˜¾æ…¢äºå¹³å‡æ°´å¹³çš„ä»»åŠ¡æ‰ä¼šè§¦å‘æ¨æµ‹æ‰§è¡Œ  
- æé«˜ä»»åŠ¡å®Œæˆæ¯”ä¾‹é˜ˆå€¼ (spark.speculation.quantile=0.9)
  éœ€è¦90%çš„ä»»åŠ¡å®Œæˆåå†å¼€å§‹åˆ¤æ–­ï¼Œç¡®ä¿æœ‰è¶³å¤Ÿæ ·æœ¬è®¡ç®—æœ‰æ„ä¹‰çš„å¹³å‡æ—¶é—´  
- spark4.0æœ€æ–°ç‰ˆæœ¬å€¼ä¹Ÿæ˜¯deepseekå»ºè®®çš„å€¼  
```
spark.speculation.multiplier	3
spark.speculation.quantile 0.9
```



### Stageè€—æ—¶å¼‚å¸¸åˆ†æ
**Stageç©ºé—²æ—¶é—´ (stageè¿è¡Œæ—¶é—´-ä»»åŠ¡è¿è¡Œæ—¶é—´) ä¸stageè¿è¡Œæ—¶é—´çš„å æ¯”è¶…è¿‡30.0%ï¼Œå³åˆ¤å®šä¸ºStageè€—æ—¶å¼‚å¸¸**
#### æ¡ˆåˆ—è§£æ
- job[1].stage[1]ç©ºé—²æ—¶é—´ä¸è¯¥Stageæ€»æ—¶é—´çš„å æ¯”ä¸ºï¼š30.59%ï¼Œè¶…è¿‡é˜ˆå€¼30.0ï¼Œè¯¥stageè€—æ—¶å¼‚å¸¸ã€‚è¯·åŠæ—¶å…³æ³¨å¹³å°è¿è¡ŒçŠ¶æ€ã€‚
- è§‚å¯Ÿåˆ°å‘ç”Ÿstageç©ºé—²æ—¶é—´å¼‚å¸¸æ—¶ï¼Œå¯¹åº”çš„sparkæ˜¾ç¤ºæœ‰was preemptedå¼‚å¸¸ï¼Œæ˜¯è§¦å‘äº†èµ„æºæŠ¢å 
- ç›®å‰é›†ç¾¤ä¸ºcapacity scheduler,æœ‰æœ€ä½èµ„æºå’Œé¢„åˆ†é…ç½®å¼¹æ€§ä½¿ç”¨ï¼Œå‘æœå ç”¨äº†é›†ç¾¤å¼¹æ€§èµ„æºï¼Œå…¶å®ƒä¸šåŠ¡éœ€è¦æœ€ä½ä¿è¯èµ„æºæ—¶ï¼Œå°±ä¼šè§¦å‘æ”¶å›å¼¹æ€§èµ„æºéƒ¨åˆ†ã€‚
![alt text](img/image-8.png)

- åŒæ—¶è¿˜è¢«è¯Šæ–­æœ‰taské•¿å°¾ï¼Œä¸»è¦æ˜¯æ•°æ®åˆ†åŒºä¸å‡è¡¡ï¼Œéƒ¨åˆ†taskä¼šæœ‰å†…å­˜æº¢å‡º,æ—¥å¿—å¦‚ä¸‹
```
java.lang.OutOfMemoryError: Java heap space
Dumping heap to /data3/hadoop/yarn/logs/application_1755049813097_2317447/container_e256_1755049813097_2317447_01_000011/executor_dump.out ...
Heap dump file created [8192319570 bytes in 11.452 secs]
#
# java.lang.OutOfMemoryError: Java heap space
# -XX:OnOutOfMemoryError="kill %p"
#   Executing /bin/sh -c "kill 6326"...
25/09/17 04:53:40 WARN TransportChannelHandler: Exception in connection from /10.12.8.90:52636
java.lang.OutOfMemoryError: Java heap space
	at java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)
	at java.nio.ByteBuffer.allocate(ByteBuffer.java:335)
	at org.apache.spark.serializer.SerializerManager.$anonfun$dataSerializeWithExplicitClassTag$1(SerializerManager.scala:192)
```
**è§£å†³æ–¹æ¡ˆ**
å‘ç°è¿™é‡Œæœ‰å¤§é‡çš„cacheï¼Œä½¿ç”¨å¤§é‡å†…å­˜ï¼Œï¼š
- sqlå†™æ³•å»æ‰ä¸å¿…è¦çš„cache,æœ‰äº›è¡¨åªç”¨äº†ä¸€æ¬¡ï¼Œæ²¡å¿…è¦åŠ cache
```sql
cache a as(...); -- cacheä¼šè§¦å‘è®¡ç®—ï¼Œå¹¶å ç”¨executorèµ„æº
-- è¿™é‡Œçœç•¥å¤§ç‰‡sqlé€»è¾‘
select * from b
left join a on b.id=a.id;  -- å†æ¬¡ä»cacheäº†æ•°æ®ï¼Œå¦‚æœæ²¡æœ‰drop cahceï¼Œä¸ä¼šä¸»åŠ¨é‡Šæ”¾cacheçš„executorèµ„æº
-- ä¼˜åŒ–å
create temporary view a as(...);  --ä¸ä¼šè§¦å‘action,ä¸ä¼šå ç”¨èµ„æº
-- è¿™é‡Œçœç•¥å¤§ç‰‡sqlé€»è¾‘
select * from b
left join a on b.id=a.id;   --è¿™é‡Œæ‰ä¼šè§¦å‘actionç®—å­è¿›è¡Œè®¡ç®—
```
- è°ƒ æœ‰å¤šæ¬¡é‡å¤æŸ¥è¯¢çš„è¡¨,ä¸ºç£ç›˜cacheã€‚ä¾‹ï¼š
```
CACHE TABLE data_source1 OPTIONS ('storageLevel' = 'DISK_ONLY') AS SELECT * FROM data_source1_view;
```
-- æ‰‹åŠ¨å¯¹æºè¡¨è¿›è¡Œåˆ†åŒº
```
-- n ä¸ºæŒ‡å®šåˆ†åŒºæ•°
/*+repartition(n) */
```
- ä¼˜åŒ–åä»æ­£å¸¸çš„sqlé€»è¾‘æ‰§è¡Œä¹Ÿä¼šå˜çŸ­ä¸€äº›ï¼Œå‘¨æ—¶ä¹Ÿä¼šæ¶ˆé™¤æ™šä¸Šé«˜å³°æœŸè¢«èµ„æºæŠ¢å å¯¼è‡´é‡ç®—çš„é£é™©ï¼Œé‡ç®—åæ—¶é—´ä¼šåœ¨20-30åˆ†é’Ÿä¸Šã€‚
![alt text](image-5.png)
![alt text](image-6.png)

### åŸºçº¿æ—¶é—´å¼‚å¸¸
ç›¸å¯¹äºå†å²æ­£å¸¸ç»“æŸæ—¶é—´ï¼Œæå‰ç»“æŸæˆ–æ™šç‚¹ç»“æŸçš„ä»»åŠ¡  

### hdfså¡é¡¿åˆ†æ  
 **è®¡ç®—Stageä¸­æ¯ä¸ªä»»åŠ¡çš„å¤„ç†é€Ÿç‡(è¯»å–æ•°æ®é‡ä¸è€—æ—¶çš„æ¯”å€¼), å½“å¤„ç†é€Ÿç‡çš„ä¸­ä½å€¼ä¸æœ€å°å€¼çš„æ¯”å¤§äº10.00,å³åˆ¤å®šä¸ºHDFSå¡é¡¿**  
 - è¿™ä¸ªçœ‹èµ·å°±æ˜¯taské•¿å°¾ï¼Œä½†è®¡ç®—ç»“æœå’Œè¿™ä¸ªå·®ä¸å¤šä¸€æ ·çš„é—®é¢˜ï¼Œè¯»ç›¸å…³å¾ˆå¤§æ˜¯æ–‡ä»¶åˆ‡ç‰‡é—®é¢˜

sparkæ—¥å¿— æœ‰äº›taskå…±ç”¨ä¸€ä¸ªexecutor,æ—¥å¿—ä¼šè¾“å‡ºåœ¨ä¸€ä¸ªæ–‡ä»¶ä¸­
![alt text](task_split.png)
```text
task 1
25/09/09 11:06:41 INFO FileScanRDD: Reading File path: hdfs://yycluster01/hive_warehouse/hiidodw.db/yy_lpfplayerfirstaccess_original/dt=20250909/hm=1029/bc_124_merge_1757385248862_0.zlib, range: 0-121518116, partition values: [20250909,1029]
25/09/09 11:06:41 INFO FileScanRDD: Reading File path: hdfs://yycluster01/hive_warehouse/hiidodw.db/yy_lpfplayerfirstaccess_original/dt=20250909/hm=1044/bc_124_merge_1757386148166_0.zlib, range: 0-121518116, partition values: [20250909,1044]
task 2
25/09/09 11:06:41 INFO FileScanRDD: Reading File path: hdfs://yycluster01/hive_warehouse/hiidodw.db/yy_lpfplayerfirstaccess_original/dt=20250909/hm=1029/bc_124_merge_1757385248862_0.zlib, range: 0-121518116, partition values: [20250909,1029]
25/09/09 11:06:41 INFO FileScanRDD: Reading File path: hdfs://yycluster01/hive_warehouse/hiidodw.db/yy_lpfplayerfirstaccess_original/dt=20250909/hm=1044/bc_124_merge_1757386148166_0.zlib, range: 0-121518116, partition values: [20250909,1044]
task 3
25/09/09 11:06:49 INFO FileScanRDD: Reading File path: hdfs://yycluster01/hive_warehouse/hiidodw.db/yy_lpfplayerfirstaccess_original/dt=20250909/hm=1014/bc_124_merge_1757384348136_0.zlib, range: 0-112774706, partition values: [20250909,1014]
25/09/09 11:06:49 INFO FileScanRDD: Reading File path: hdfs://yycluster01/hive_warehouse/hiidodw.db/yy_lpfplayerfirstaccess_original/dt=20250909/hm=1059/bc_124_merge_1757387049011_0.zlib, range: 0-109471027, partition values: [20250909,1059]
task 4
25/09/09 11:06:49 INFO FileScanRDD: Reading File path: hdfs://yycluster01/hive_warehouse/hiidodw.db/yy_lpfplayerfirstaccess_original/dt=20250909/hm=1014/bc_124_merge_1757384348136_0.zlib, range: 0-112774706, partition values: [20250909,1014]
25/09/09 11:06:49 INFO FileScanRDD: Reading File path: hdfs://yycluster01/hive_warehouse/hiidodw.db/yy_lpfplayerfirstaccess_original/dt=20250909/hm=1059/bc_124_merge_1757387049011_0.zlib, range: 0-109471027, partition values: [20250909,1059]
task 5
25/09/09 11:06:50 INFO FileScanRDD: Reading File path: hdfs://yycluster01/hive_warehouse/hiidodw.db/yy_lpfplayerfirstaccess_original/dt=20250909/hm=1044/bc_124_merge_1757386148166_0.zlib, range: 121518116-123307681, partition values: [20250909,1044]
```
#### åŸå› åˆ†æ

- æ–‡ä»¶æ ¼å¼ï¼šORC + zlibå‹ç¼©
- æ–‡ä»¶å¤§å°ï¼š123,307,681å­—èŠ‚ (çº¦117MB)
- HDFSå—å¤§å°ï¼š268,435,456å­—èŠ‚ (256MB)
- è¡Œæ•°ï¼š586,304è¡Œ
- **è¿™é‡Œçœç•¥ä¸€å¤§å †çš„è°ƒè¯•åˆ†æ....**

**çœŸæ­£åŸå› ï¼š**
- é€ æˆtaskè¿”å›nullå€¼çš„åŸå› æ˜¯åŸå§‹æ–‡ä»¶æ•°æ®å€¾æ–œï¼Œåˆ‡åˆ°æ–‡ä»¶æœ«å°¾æœ€åä¸€ç‚¹æ˜¯ç©ºè¡Œæ•°æ®ï¼Œ spark QAEä¸»è¦æ˜¯åœ¨shuufle joinä¸­è¿›è¡Œé‡æ–°åˆ†åŒºä¼˜åŒ–ï¼Œå¯¹äºåŸå§‹æ•°æ®å€¾æ–œæ˜¯æ²¡æœ‰æ•ˆæœçš„ã€‚
- sparkåˆ‡å—é€»è¾‘ï¼ŒæŒ‰åˆ‡å—æ¥åˆ†åŒºè¿›è¡Œtask
```text
20250910  10ç‚¹åˆ†åŒºæ–‡ä»¶
hdfs dfs -du    hdfs://yycluster01/hive_warehouse/xx.db/yy_lpfplayerfirstaccess_original/dt=20250909/hm=10*
112774706  338324118  hdfs://yycluster01/hive_warehouse/xx.db/yy_lpfplayerfirstaccess_original/dt=20250909/hm=1014/bc_124_merge_1757384348136_0.zlib
123741835  371225505  hdfs://yycluster01/hive_warehouse/xx.db/yy_lpfplayerfirstaccess_original/dt=20250909/hm=1029/bc_124_merge_1757385248862_0.zlib
123307681  369923043  hdfs://yycluster01/hive_warehouse/xx.db/yy_lpfplayerfirstaccess_original/dt=20250909/hm=1044/bc_124_merge_1757386148166_0.zlib
109471027  328413081  hdfs://yycluster01/hive_warehouse/xx.db/yy_lpfplayerfirstaccess_original/dt=20250909/hm=1059/bc_124_merge_1757387049011_0.zlib

minPartitionNum=leafNodeDefaultParallelism -> spark.default.parallelism= -> (execoutor * core) -> 4
469295249+4194304*4=486072465
520249602/4=121518116.25

    val defaultMaxSplitBytes = sparkSession.sessionState.conf.filesMaxPartitionBytes
    val openCostInBytes = sparkSession.sessionState.conf.filesOpenCostInBytes
    val minPartitionNum = sparkSession.sessionState.conf.filesMinPartitionNum
      .getOrElse(sparkSession.leafNodeDefaultParallelism)
    val totalBytes = selectedPartitions.flatMap(_.files.map(_.getLen + openCostInBytes)).sum
    val bytesPerCore = totalBytes / minPartitionNum
    Math.min(defaultMaxSplitBytes, Math.max(openCostInBytes, bytesPerCore))
	
defaultMaxSplitBytes:256MB
Math.min(256MB, Math.max(4096, 121518116))
```

- æœ€ç»ˆsparkæ—¥å¿—è¾“å‡ºä¸ºï¼šrange: 0-121518116  

**ä¼˜åŒ–æ–¹æ¡ˆ**
ä¾‹ï¼š 
```sql
select c1,c2...å„ç§åˆ—ç»„åˆè½¬æ¢ from tab where dt=xx ..
group by cl,c2...;
-- ä¼˜åŒ–å å¯¹åŸå§‹è¡¨è¿›è¡Œé‡åˆ†åŒºï¼Œ/*+ REPARTITION(12) */ æ˜¯ä¸€ä¸ªæ˜ç¡®çš„ä¼˜åŒ–å™¨æç¤ºï¼Œå‘Šè¯‰ Spark å¿…é¡»å°†æ•°æ®é‡æ–°åˆ†åŒºä¸º 12 ä¸ªåˆ†åŒº
-- å°æç¤º.æ…ç”¨è¿™ä¸ªï¼šDISTRIBUTE BY CAST(rand() * 24 AS INT) æ˜¯ä¸€ä¸ªé€»è¾‘è¡¨è¾¾å¼ï¼ŒSpark çš„ä¼˜åŒ–å™¨å¯èƒ½ä¼šå¯¹å…¶è¿›è¡Œä¼˜åŒ–æˆ–é‡å†™
create temporary table tmp_tab  as
select  select /*+ REPARTITION(24) */
* from tab where dt=xx ..;
select c1,c2...å„ç§åˆ—ç»„åˆè½¬æ¢ from tmp_tab where dt=xx ..
group by cl,c2...;
```

**æ•ˆæœ**
- æ²¡æœ‰ä¼˜åŒ–å‰ï¼Œtaskåªæœ‰5ä¸ªï¼Œå…¶ä¸­ä¸€ä¸ªæ˜¯ç©ºå€¼ï¼Œinputæ•°æ®ä¹Ÿç›¸å…³å¾ˆå¤§ï¼Œä¼˜åŒ–å3.6 MiB / 19516	æ•°æ®å¾ˆå¹³å‡ï¼Œåˆ†åŒºæ•°éœ€è¦æ ¹æ®è‡ªå·²æ•°æ®é‡æ¥è°ƒæ•´
![alt text](img/task_03.png)

### Jobè€—æ—¶å¼‚å¸¸åˆ†æ
**Jobä¸­ç©ºé—²æ—¶é—´ (jobæ€»æ—¶é—´ - stageç´¯è®¡æ—¶é—´) ä¸æ€»æ—¶é—´çš„å æ¯”è¶…è¿‡30.00%%ï¼Œå³åˆ¤å®šä¸ºJobè€—æ—¶å¼‚å¸¸**



## å¼‚å¸¸æ’åç»Ÿè®¡ 
 - è¯Šæ–­ç»“æœå­˜åœ¨ES,éæ ‡å‡†ç»Ÿä¸€æ ¼å¼çš„jsonï¼Œå¾ˆéš¾é€šè¿‡ES sqlç»Ÿè®¡å‡ºæ¥ï¼Œå¿è¯•ç”¨spark read ESç›´æ¥åˆ†æï¼Œsparkæ¨åˆ°ç»“æ„å¤±è´¥ï¼Œjsonç»“æ„è¿‡äºå¤æ‚å’Œä¸ä¸€è‡´å¯¼è‡´ã€‚  
 - è¿™é‡Œåªèƒ½é€šè¿‡å¯¼å‡ºES jsonæ–‡ä»¶åˆ°HDFSä¸Šï¼Œsparkè¯»å–HDFS jsonæ–‡ä»¶è¿›è¡Œè§£æï¼Œè§£æä»£ç å¦‚ä¸‹ï¼š  
![alt text](image-3.png)

<div class="code-collapse-header" onclick="toggleCodeCollapse(this)">ç‚¹å‡»å±•å¼€sparkè§£æä»£ç  <span class="toggle-icon">â–¼</span></div>

```scala
package com.aengine.spark.app.compass
import com.aengine.spark.utils.ResourcesUtils
import org.apache.commons.lang.StringUtils
import org.apache.spark.sql.{DataFrame, SparkSession, functions}
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types.{ArrayType, BooleanType, FloatType, IntegerType, LongType, StringType, StructField, StructType}
import java.text.SimpleDateFormat
import java.util.{Calendar, Locale}

/**
 * read ES JSON DATA parese json data to detect result order by max values TOP_N
 * create by liangrui on 20250904
 *
 * es json data to hdfs by spark  stat top
 * es_date=2025-09-07
 * elasticdump   --input=http://xx:2949/compass-detector-app-$es_date --output=/data/es/compass-detector-app-$es_date.json  --type=data
 * hdfs dfs -put  /data/es/compass-detector-app-$es_date.json /data/es/
 *
 */
object ReadEsJosnFile {
  //è§£æjsonè§„åˆ™
  //dataSewk  -> ratio
  //stageDurationAbnormal  -> ratio
  //jobDurationAbnormal  -> ratio
  //taskDurationAbnormal  -> ratio
  //hdfsStuck  -> ratio
  //speculativeTask  -> speculativeCount
  //largeTableScan   -> rows


  def main(args: Array[String]): Unit = {

    var Array(argDate, other) = (args ++ Array(null, null)).slice(0, 2)
    val dateFormatter = new SimpleDateFormat("yyyy-MM-dd", Locale.US)
    val calendar = Calendar.getInstance
    //calendar.add(Calendar.HOUR_OF_DAY, -1) // å‡å»1å°æ—¶
    val currentDate = dateFormatter.format(calendar.getTime)
    var es_index_file = ""
    if (StringUtils.isBlank(argDate)) {
      es_index_file = s"/data/es/compass-detector-app-$currentDate.json"
      argDate = currentDate
    } else {
      es_index_file = s"/data/es/compass-detector-app-$argDate.json"
    }
    println("es_index:" + es_index_file)
    val spark = SparkSession.builder()
      .appName("com.aengine.spark.app.compass.SparkReadES")
      .getOrCreate()
    val df = spark.read.json(es_index_file)
    df.printSchema()
    df.show()
    val newDF = df.select("_source.applicationId", "_source.taskName", "_source.executionTime", "_source.dataList", "_source.projectName", "_source.flowName")
      .filter(col("dataList").isNotNull && functions.size(col("dataList")) > 0)

    val df_parsed = newDF.select(
      col("applicationId"),
      col("taskName"),
      col("executionTime"),
      col("projectName"),
      col("flowName"),
      explode(col("dataList")).alias("dataListItem"))

    //# æå–dataListä¸­çš„å­—æ®µ
    // é¦–å…ˆæ£€æŸ¥dataListItemçš„schemaæ¥ç¡®å®štableså­—æ®µæ˜¯å¦å­˜åœ¨
    val dataListItemColumns = df_parsed.select(col("dataListItem")).schema.head.dataType.asInstanceOf[org.apache.spark.sql.types.StructType].fieldNames.toSet

    val expandedDF = if (dataListItemColumns.contains("tables")) {
      // å¦‚æœtableså­—æ®µå­˜åœ¨ï¼Œæ­£å¸¸æå–
      df_parsed.select(
        col("applicationId"),
        col("taskName"),
        col("executionTime"),
        col("projectName"),
        col("flowName"),
        col("dataListItem.abnormal").alias("abnormal"),
        col("dataListItem.appCategory").alias("appCategory"),
        col("dataListItem.data").alias("data"),
        col("dataListItem.tables").alias("tables")
      )
    } else {
      // å¦‚æœtableså­—æ®µä¸å­˜åœ¨ï¼Œæ·»åŠ ç©ºæ•°ç»„
      df_parsed.select(
        col("applicationId"),
        col("taskName"),
        col("executionTime"),
        col("projectName"),
        col("flowName"),
        col("dataListItem.abnormal").alias("abnormal"),
        col("dataListItem.appCategory").alias("appCategory"),
        col("dataListItem.data").alias("data"),
        lit(null).cast("string").alias("tables")
      )
    }
    println("expandedDF print====")
    expandedDF.printSchema()
    expandedDF.show()
    // å®šä¹‰Schema
    val dataSkewSchema = StructType(Seq(
      StructField("abnormal", BooleanType, true),
      // StructField("attemptNumber", LongType, true),
      //StructField("duration", LongType, true),
      // StructField("jobId", LongType, true),
      // StructField("maxShuffleReadBytes", LongType, true),
      //StructField("maxShuffleReadRecords", LongType, true),
      //StructField("medianRecords", LongType, true),
      StructField("ratio", FloatType, true),
      //StructField("stageId", FloatType, true),
      StructField("threshold", FloatType, true)
    ))
    val dataSchema = ArrayType(dataSkewSchema) // å®šä¹‰ä¸ºæ•°ç»„ç±»å‹
    // ration column çš„è§£æé€»è¾‘
    val RatioDF = expandedDF.
      filter(
        """
          |abnormal=true
          | and appCategory
          | in('dataSkew','stageDurationAbnormal','jobDurationAbnormal','taskDurationAbnormal','hdfsStuck') """
          .stripMargin).
      withColumn("data_array", from_json(col("data"), dataSchema,
        Map("mode" -> "PERMISSIVE", "allowNumericLeadingZeros" -> "true")))
    val RatioResultDF = getRatioDF(spark, RatioDF, "ratio")

    // rows column çš„è§£æé€»è¾‘
    val rowsStructSchema = StructType(Seq(
      StructField("abnormal", BooleanType, true),
      StructField("rows", LongType, true),
      StructField("threshold", FloatType, true)
    ))
    val rowsSchema = ArrayType(rowsStructSchema) // å®šä¹‰ä¸ºæ•°ç»„ç±»å‹
    val RowsDF = expandedDF.filter("abnormal=true and appCategory = 'largeTableScan'").
      withColumn("data_array", from_json(col("data"), rowsSchema,
        Map("mode" -> "PERMISSIVE", "allowNumericLeadingZeros" -> "true")))
    val RowsResultDF = getRatioDF(spark, RowsDF, "rows")

    // speculativeCount column çš„è§£æé€»è¾‘
    val speculativeCountStructSchema = StructType(Seq(
      StructField("abnormal", BooleanType, true),
      StructField("speculativeCount", IntegerType, true),
      StructField("threshold", FloatType, true)
    ))
    val speculativeCountSchema = ArrayType(speculativeCountStructSchema) // å®šä¹‰ä¸ºæ•°ç»„ç±»å‹
    val speculativeCountDF = expandedDF.filter("abnormal=true and appCategory = 'speculativeTask'").
      withColumn("data_array", from_json(col("data"), speculativeCountSchema,
        Map("mode" -> "PERMISSIVE", "allowNumericLeadingZeros" -> "true")))
    val speculativeCountResultDF = getRatioDF(spark, speculativeCountDF, "speculativeCount")

    //union
    val ResultDF = RatioResultDF.union(RowsResultDF).union(speculativeCountResultDF).
      withColumn("es_index", lit(argDate)).
      withColumn("create_date", lit(currentDate))
    ResultDF.show(false)

    //wirter mysql
    val jdbc_conf = ResourcesUtils.getProperties("jdbc.properties")
    val targetJdbcUrl = jdbc_conf.getProperty("compass_url")
    val targetJdbcUsername = jdbc_conf.getProperty("compass_user")
    val targetJdbcPassword = jdbc_conf.getProperty("compass_password")
    //delete
    deleteMysql(targetJdbcUrl, targetJdbcUsername, targetJdbcPassword, argDate)
    ResultDF.write
      .format("jdbc")
      .option("url", targetJdbcUrl)
      .option("driver", "com.mysql.jdbc.Driver")
      .option("dbtable", "detect_top")
      .option("user", targetJdbcUsername)
      .option("password", targetJdbcPassword)
      .mode("append")
      .save()

    spark.stop()
  }


  /**
   * get  column max value order by to top
   *
   * @param spark
   * @param RatioDF
   * @return
   */
  def getRatioDF(spark: SparkSession, RatioDF: DataFrame, columnName: String): DataFrame = {
    println("finalDF print====")
    RatioDF.printSchema()
    RatioDF.show(10, false)
    RatioDF.select(
      col("applicationId"),
      col("taskName"),
      col("executionTime"),
      col("projectName"),
      col("flowName"),
      col("abnormal"),
      col("appCategory"),
      explode(col("data_array")).alias("data_element")
    ).select(
      col("applicationId"),
      col("taskName"),
      col("executionTime"),
      col("projectName"),
      col("flowName"),
      col("abnormal"),
      col("appCategory"),
      col("data_element.abnormal").alias("data_abnormal"), // æå–ç»“æ„ä½“å†…çš„å­—æ®µ
      //col(s"data_element.$columnName").alias("data_col"),
      col(s"data_element.$columnName").cast("float").alias("data_col"),
      col("data_element.threshold").alias("threshold"),
      // col("data_element.jobId").alias("jobId"),
      //col("data_element.stageId").alias("stageId")
    ).filter("data_abnormal=true").createOrReplaceTempView("tmp_result")

    spark.sql(
      s"""
         |with ranked_data as (
         |  select
         |    projectName as project_name,
         |    flowName as flow_name,
         |    taskName as task_name,
         |    applicationId as application_id,
         |    executionTime as execution_time,
         |    appCategory as app_category,
         |    ROUND(max(data_col),2) as max_detect,
         |    ROUND(sum(data_col),2) as sum_detect,
         |    ROW_NUMBER() OVER (PARTITION BY appCategory ORDER BY ROUND(max(data_col),2) DESC) as rn
         |  from tmp_result
         |  group by
         |    projectName,flowName,taskName,applicationId,executionTime,appCategory
         |)
         |select 
         |  project_name,
         |  flow_name,
         |  task_name,
         |  application_id,
         |  execution_time,
         |  app_category,
         |  max_detect,
         |  sum_detect
         |from ranked_data
         |where rn <= 100
         |order by app_category, max_detect desc
         |""".stripMargin)
  }

  def deleteMysql(jdbcUrl: String, jdbcUsername: String, jdbcPassword: String, es_index: String): Unit = {
    import java.sql.{Connection, DriverManager, PreparedStatement}
    var connection: Connection = null
    var preparedStatement: PreparedStatement = null

    try {
      // æ³¨å†Œé©±åŠ¨ï¼Œå»ºç«‹è¿æ¥
      Class.forName("com.mysql.jdbc.Driver")
      //Class.forName("com.mysql.cj.jdbc.Driver")
      println(s"Connecting to database: $jdbcUrl")
      connection = DriverManager.getConnection(jdbcUrl, jdbcUsername, jdbcPassword)
      println("Database connection established successfully.")

      // æ„å»ºå‚æ•°åŒ–çš„DELETE SQLè¯­å¥
      val sql = "DELETE FROM detect_top WHERE es_index = ?"
      println(s"Preparing SQL: $sql with parameter: $es_index")
      preparedStatement = connection.prepareStatement(sql)

      // è®¾ç½®å‚æ•°
      preparedStatement.setString(1, es_index)

      // æ‰§è¡Œåˆ é™¤ï¼ˆä¸ä½¿ç”¨äº‹åŠ¡ï¼Œç›´æ¥æäº¤ï¼‰
      val affectedRows = preparedStatement.executeUpdate()
      println(s"Delete operation completed. Affected rows: $affectedRows")

      if (affectedRows > 0) {
        println(s"Successfully deleted $affectedRows rows from detect_top where es_index = '$es_index'")
      } else {
        println(s"No rows found with es_index = '$es_index' in detect_top table")
      }
    } catch {
      case e: Exception =>
        println(s"Error occurred during delete operation: ${e.getMessage}")
        e.printStackTrace()
    } finally {
      // å…³é—­èµ„æº
      if (preparedStatement != null) preparedStatement.close()
      if (connection != null) connection.close()
    }
  }
}  

```
{: .scala-code-block style="display: none;"}

<script src="/assets/blog.js"></script>
<link rel="stylesheet" href="/assets/blog.css">



  <nav class="blog-nav">
    <button class="collapse-btn" onclick="toggleBlogNav()">â˜°</button>
    {% include blog_navigation.html items=site.data.blog_navigation %}
</nav>