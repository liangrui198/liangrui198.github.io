---
layout: default
title:  hdfs cache
author: liangrui
description: "hdfs cache" 
keywords: hadoop,hdfs,HDFS MemoryStorage,CentralizedCacheManagement
date: 2025-11-20
---

# hdfsé›†ä¸­å¼ç¼“å­˜
hdfsä¹Ÿæ”¯æŒç¼“å­˜é…ç½®ï¼ŒæŠŠå†…å­˜å½“ä½œç£ç›˜ä¸€æ ·è¯»å†™æ•°æ®ï¼Œç±»ä¼¼alluxio cache,è¿™ä¸ªæ˜¯åŸºäºhdfså†…éƒ¨ç®¡ç†çš„ï¼Œå¦‚æœæŸäº›hdfsæ–‡ä»¶é«˜ioçš„tmpæ—¶æ–‡ä»¶ï¼Œæ˜¯ä¸€ä¸ªå¾ˆå¥½çš„é€‰æ‹©ã€‚  
å†…å­˜å­˜å‚¨ï¼šhttps://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/MemoryStorage.html
å®˜æ–¹æ–‡æ¡£ï¼šhttps://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/CentralizedCacheManagement.html


## é…ç½®ç›¸å…³
### å†…å­˜å­˜å‚¨
**å¿…é¡»é…ç½® æ­¤å‚æ•°å†³å®šäº† DataNode ç”¨äºç¼“å­˜çš„æœ€å¤§å†…å­˜é‡ï¼Œå­—èŠ‚ä¸ºå•ä½æŒ‡å®š.**     
`dfs.datanode.max.locked.memory=34359738368`  
**æŒ‚è½½ä¸€ä¸ª 32 GB çš„tmpfsåˆ†åŒºæˆ–è€…ç›´æ¥ç”¨ç³»ç»Ÿä¸Šå·²æŒ‚è½½çš„tmpåˆ†åŒº**    
`sudo mount -t tmpfs -o size=32g tmpfs /mnt/dn-tmpfs/`   
**æˆ–è€…(ç³»ç»Ÿä¸ä¸€æ ·ï¼ŒæŒ‚è½½çš„åå­—ä¹Ÿä¼šä¸ä¸€æ ·ï¼Œæ ¹æ®è‡ªå·±çš„ç³»ç»Ÿé€‰æ‹©)**       
```
df -h  
tmpfs            63G     0   63G   0% /dev/shm  
```
**tmpfsä½¿ç”¨ RAM_DISK å­˜å‚¨ç±»å‹æ ‡è®°å·**   
```xml
    <property>
      <name>dfs.datanode.data.dir</name>
      <value>/data1/x,/data1/x,/data3/x,[RAM_DISK]/dev/shm</value>
    </property>
```


### é›†ä¸­å¼ç¼“å­˜ç®¡ç†
**å¿…é¡»é…ç½® æ­¤å‚æ•°å†³å®šäº† DataNode ç”¨äºç¼“å­˜çš„æœ€å¤§å†…å­˜é‡ï¼Œå­—èŠ‚ä¸ºå•ä½æŒ‡å®š.**   
`dfs.datanode.max.locked.memory=34359738368`
//è¿™ä¸ªé…ç½®æ•°è¦ä½äºlinuxä¸Šçš„(ulimit -l)çš„å€¼ï¼Œè¿™ä¸ªå‚æ•°æ§åˆ¶è¿›ç¨‹å¯ä»¥å°†å¤šå°‘å†…å­˜é”å®šåœ¨ç‰©ç†RAMä¸­ï¼Œé˜²æ­¢è¢«äº¤æ¢åˆ°ç£ç›˜ã€‚ä¾‹ï¼š
`max locked memory       (kbytes, -l) 64`

é»˜è®¤å¤ªå°ï¼Œtmpä¿®æ”¹åˆ°32g  
`ulimit -l 33554432`
æ°¸ä¹…ä¿®æ”¹ ï¼ˆéœ€è¦rootæƒé™ï¼‰ ç¼–è¾‘   
`echo -e "\nhdfs soft memlock 33554432\nhdfs hard memlock 33554432" >> /etc/security/limits.d/hdfs.conf`
æŸ¥çœ‹è¿™ä¸ªå€¼  
`ulimit -l`


å¯åŠ¨æ—¥å¿—æ˜¾ç¤ºCannot start datanode because the configured max locked memory size é—®é¢˜   
```
2025-12-08 14:32:21,168 ERROR datanode.DataNode (DataNode.java:secureMain(2883)) - Exception in secureMain
java.lang.RuntimeException: Cannot start datanode because the configured max locked memory size (dfs.datanode.max.locked.memory) of 3612361255 bytes is more than the datanode's available RLIMIT_MEMLOCK ulimit of 65536 bytes.
        at org.apache.hadoop.hdfs.server.datanode.DataNode.startDataNode(DataNode.java:1389)
        at org.apache.hadoop.hdfs.server.datanode.DataNode.<init>(DataNode.java:500)
        at org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:2782)
        at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:2690)
        at org.apache.hadoop.hdfs.server.datanode.DataNode.createDataNode(DataNode.java:2732)
        at org.apache.hadoop.hdfs.server.datanode.DataNode.secureMain(DataNode.java:2876)
        at org.apache.hadoop.hdfs.server.datanode.SecureDataNodeStarter.start(SecureDataNodeStarter.java:100)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.commons.daemon.support.DaemonLoader.start(DaemonLoader.java:243)

hdfs@on-test-hadoop-65-239:/home/liangrui06$ cat /proc/34131/limits
Limit                     Soft Limit           Hard Limit           Units     
Max locked memory         65536                65536                bytes     
...    
```
**è§£å†³æ–¹æ¡ˆ**   

1ï¼šambari ä¸­çš„hadoop-env.shæ¨¡æ¿ä¹Ÿæœ‰ç‚¹é—®é¢˜, "$command" == "datanode" è¿™ä¸ªå˜é‡æˆ‘çš„ç¯å¢ƒæ˜¯ç©ºçš„ï¼Œéœ€è¦å»æ‰   
```
 # Fix temporary bug, when ulimit from conf files is not picked up, without full relogin.
 # Makes sense to fix only when runing DN as root
 #  if [ "$command" == "datanode" ] && [ "$EUID" -eq 0 ] && [ -n "$HDFS_DATANODE_SECURE_USER" ]; then
 if [ "$EUID" -eq 0 ] && [ -n "$HDFS_DATANODE_SECURE_USER" ]; then
 {% if is_datanode_max_locked_memory_set %}
 ulimit -l {{datanode_max_locked_memory}}
 {% endif %}
 ulimit -n {{hdfs_user_nofile_limit}}
 fi
```

2ï¼šåªä¿®æ”¹ä¸Šè€Œé‚£ä¸ªæ–‡ä»¶ä¸è¡Œï¼Œå‘ç°åœ¨ambariä¸­å¯åŠ¨çš„æ—¶å€™ï¼Œä¼šè¦†ç›–è¿™ä¸ªç©æ„ï¼ŒåŸæ¥æ˜¯ambariå†…éƒ¨æœ‰ä¸ªetcé…ç½®æ–‡ä»¶ï¼Œéœ€è¦æ‰‹åŠ¨ä¿®æ”¹ï¼Œåœ¨é¡µé¢ä¸Šæ‰¾ä¸åˆ°å…¥å£ä¿®æ”¹,å•ä½æ˜¯kbã€‚

`echo -e "\n{{hdfs_user}}   - memlock {{hdfs_user_memlock_limit|default(33554432)}}" >> /data/ambari-agent/cache/stacks/HDP/3.0/services/HDFS/package/templates/hdfs.conf.j2`  
æˆ–    
`echo -e "\n*   - memlock {{hdfs_user_memlock_limit|default(33554432)}}" >> /data/ambari-agent/cache/stacks/HDP/3.0/services/HDFS/package/templates/hdfs.conf.j2`    
ambari-serviceæ–‡ä»¶ä¹Ÿéœ€è¦æ›´æ”¹   
`echo -e "\n*   - memlock {{hdfs_user_memlock_limit|default(33554432)}}" >> /var/lib/ambari-server/resources/stacks/HDP/3.0/services/HDFS/package/templates/hdfs.conf.j2`

è¿™ä¸ªé…ç½®æµ‹è¯•æ²¡æœ‰ç”¨åˆ°ï¼Œmarkä¸€ä¸‹       
`echo -e "\nhdfs soft memlock 33554432\nhdfs hard memlock 33554432" >> /usr/hdp/3.1.0.0-78/etc/security/limits.d/hdfs.conf`

3ï¼šå¦‚æœè¿˜ä¸è¡Œï¼Œå°±ç”¨é€šé…ç¬¦é…ç½®
```
echo "* - memlock 33554432" | sudo tee -a /etc/security/limits.conf
```

4:éªŒè¯è¿›ç¨‹çš„locked memory
```
cat /proc/${PID}$/limits | grep 'locked memory'
Max locked memory         35184372088832       35184372088832       bytes     
```


## cacheå‘½ä»¤
```
# æ–°å»ºpool å’Œè·¯å¾„
hdfs cacheadmin -addPool p001
hdfs cacheadmin -addDirective -path /cache/001 -pool p001 
hdfs cacheadmin -addDirective -path /cache/002  -pool p001  -replication 1 -ttl 1h
hdfs dfs -mkdir  /cache/001

# æŸ¥çœ‹
hdfs cacheadmin -listDirectives 
Found 1 entry
 ID POOL   REPL EXPIRY  PATH       
  1 p001      1 never   /cache/001

hdfs cacheadmin -listDirectives  -stats  
hdfs cacheadmin -listPools -stats  

# é€šè¿‡listæŸ¥çœ‹id è¿›è¡Œåˆ é™¤
hdfs cacheadmin -removeDirective id  
hdfs cacheadmin -removeDirectives <path>
```


## éªŒè¯
  
æ‰“å¼€æœåŠ¡èŠ‚ç‚¹åï¼Œåœ¨æ˜¾ç¤ºå­˜å‚¨ä¿¡æ¯æ—¶ï¼Œä¼šæœ‰å†…å­˜å­˜å‚¨ä¿¡æ¯   
![alt text](img/image-1.png)  

åœ¨cacheä¸­putä¸€ä¸ªæ–‡ä»¶  
```bash
hdfs dfs -put mysql-connector-java-5.1.49.jar /cache/002/
hdfs dfs -cat /cache/002/mysql-connector-java-5.1.49.jar > /dev/null 2>&1
```


æŸ¥çœ‹cacheä¸­çš„ç»Ÿè®¡   
```
hdfs cacheadmin -listDirectives -stats
Found 2 entries
 ID POOL   REPL EXPIRY                    PATH         BYTES_NEEDED  BYTES_CACHED  FILES_NEEDED  FILES_CACHED
  1 p001      1 never                     /cache/001        1006904       1006904             1             1
  2 p001      1 2025-12-09T16:02:01+0800  /cache/002        1006904       1006904             1             1
```

æŸ¥çœ‹èŠ‚ç‚¹Cache Used  
`hdfs dfsadmin -report`   
```
... 
Name: 10.12.65.x:1019 (on-test-hadoop-65-239.x.x.x.x.com)
Hostname: on-test-hadoop-65-239.hiido.host.int.yy.com
Rack: /4F08-06-04
Decommission Status : Normal
Configured Capacity: 41337038389248 (37.60 TB)
DFS Used: 1463606235771 (1.33 TB)
Non DFS Used: 0 (0 B)
DFS Remaining: 37472759803269 (34.08 TB)
DFS Used%: 3.54%
DFS Remaining%: 90.65%
Configured Cache Capacity: 34359738368 (32 GB)
Cache Used: 2015232 (1.92 MB)
Cache Remaining: 34357723136 (32.00 GB)
Cache Used%: 0.01%
Cache Remaining%: 99.99%
Xceivers: 2
Last contact: Tue Dec 09 15:08:33 CST 2025
Last Block Report: Tue Dec 09 14:51:33 CST 2025
Num of Blocks: 554506
```  
å¯ä»¥çœ‹åˆ° Configured Cache Capacity: 34359738368 (32 GB)  Cache Used: 2015232 (1.92 MB)  ç¬¦åˆé¢„æœŸ



<div class="post-date">
  <span class="calendar-icon">ğŸ“…</span>
  <span class="date-label">å‘å¸ƒï¼š</span>
  <time datetime="2025-12-10" class="date-value">2025-12-10</time>
</div>

<div class="outline" style="background:#f6f8fa;padding:1em 1.5em 1em 1.5em;margin-bottom:1em;border-radius:8px;">
  <strong>å¤§çº²ï¼š</strong>
  <ul id="outline-list" style="margin:0;padding-left:1.2em;"></ul>
</div>

<!--èœå•æ -->
  <nav class="blog-nav">
    <button class="collapse-btn" onclick="toggleBlogNav()">â˜°</button>
    {% include blog_navigation.html items=site.data.blog_navigation %}
 </nav>

 <script src="/assets/blog.js"></script>
<link rel="stylesheet" href="/assets/blog.css">